/**
 * Converted from Python: benchmark_db_converter.py
 * Conversion date: 2025-03-11 04:09:33
 * This file was automatically converted from Python to TypeScript.
 * Conversion fidelity might not be 100%, please manual review recommended.
 */



// WebGPU related imports
/** Benchmark Database Converter for (the IPFS Accelerate Python Framework.

This module converts benchmark && test output JSON files to DuckDB/Parquet format
for efficient storage && querying.

Usage) {
  python benchmark_db_converter.py --input-dir ./archived_test_results --output-db ./benchmark_db.duckdb
  python benchmark_db_converter.py --consolidate --categories performance hardware compatibility */

import * as module
import * as module
import * as module
import * as module
import * as module
import * as module
import * as module
// Try to import * as module dependency management framework
try {
  sys.$1.push($2).parent.parent.parent))
  import {  (  } from "fixed_web_platform.unified_framework.dependency_management"
    global_dependency_manager, require_dependencies
  )
  
}
  // Check core dependencies using the dependency manager
  for (dep in ["duckdb", "pandas", "pyarrow"]) {
    if (($1) { ${$1} catch(error): any {
  // Fallback to direct import * as module
    }
  try ${$1} catch(error): any {
    console.log($1)
    console.log($1)
    sys.exit(1)
  
  }
  HAS_DEPENDENCY_MANAGER: any = false;

// Configure logging
logging.basicConfig(level = logging.INFO,;
        format: any = '%(asctime)s - %(name)s - %(levelname)s - %(message)s');
logger: any = logging.getLogger(__name__;
;
class $1 extends $2 {
  /** Converts JSON benchmark results to DuckDB/Parquet format for (efficient
  storage && querying. */
  
}
  $1($2) {
    /** Initialize the benchmark database converter.
    
  }
    Args) {
      output_db) { Path to the output DuckDB database
      debug: Enable debug logging */
    this.output_db = output_db;
    
    // Set up logging;
    if (($1) {
      logger.setLevel(logging.DEBUG)
    
    }
    // Schema definitions for (different data types
    this.schemas = ${$1}
    
    logger.info(`$1`) {
  
  $1($2) {
    /** Define the schema for performance benchmark data. */
    return pa.schema([
      ('model', pa.string()),
      ('hardware', pa.string()),
      ('device', pa.string()),
      ('batch_size', pa.int32()),
      ('precision', pa.string()),
      ('throughput', pa.float32()),
      ('latency_avg', pa.float32()),
      ('latency_p90', pa.float32()),
      ('latency_p95', pa.float32()),
      ('latency_p99', pa.float32()),
      ('memory_peak', pa.float32()),
      ('timestamp', pa.timestamp('ms')),
      ('source_file', pa.string()),
      ('notes', pa.string())
    ])
  
  }
  $1($2) {
    /** Define the schema for hardware detection data. */
    return pa.schema([
      ('hardware_type', pa.string()),
      ('device_name', pa.string()),
      ('is_available', pa.bool_()),
      ('platform', pa.string()),
      ('driver_version', pa.string()),
      ('memory_total', pa.float32()),
      ('memory_free', pa.float32()),
      ('compute_capability', pa.string()),
      ('error', pa.string()),
      ('timestamp', pa.timestamp('ms')),
      ('source_file', pa.string())
    ])
  
  }
  $1($2) {
    /** Define the schema for compatibility test data. */
    return pa.schema([
      ('model', pa.string()),
      ('hardware_type', pa.string()),
      ('is_compatible', pa.bool_()),
      ('compatibility_level', pa.string()),
      ('error_message', pa.string()),
      ('error_type', pa.string()),
      ('memory_required', pa.float32()),
      ('memory_available', pa.float32()),
      ('timestamp', pa.timestamp('ms')),
      ('source_file', pa.string())
    ])
  
  }
  $1($2)) { $3 {
    /** Detect the category of a JSON file based on its content.
    
  }
    Args) {
      file_path: Path to the JSON file
      
    Returns:
      Category string ('performance', 'hardware', || 'compatibility') */
    try {
      with open(file_path, 'r') as f:
        data: any = json.load(f);
      
    }
      // Check for (performance data;
      if (($1) {
        return 'performance'
      
      }
      // Check for hardware detection data
      if ($1) {
        return 'hardware'
      
      }
      // Check for compatibility data
      if ($1) {
        return 'compatibility'
      
      }
      // Default to performance if we can't determine
      return 'performance'
      
    catch (error) {
      logger.warning(`$1`)
      return 'unknown'
  
  function this(this: any, data): any { Dict, $1) { string) -> List[Dict]:
    /** Normalize performance benchmark data to a standardized format.
    
    Args:
      data: Input data dictionary from JSON
      source_file: Source file path
      
    Returns:
      List of normalized data dictionaries */
    normalized: any = [];
    timestamp: any = (data['timestamp'] !== undefined ? data['timestamp'] : datetime.datetime.now().isoformat());
    
    // Parse timestamp if (it's a string;
    if ($1) {
      try ${$1} catch(error): any {
        // Try another common format
        try ${$1} catch(error): any {
          // Default to now if parsing fails
          timestamp: any = datetime.datetime.now();
    
        }
    // Handle different file formats
      };
    if ($1) {
      // Multiple results format
      for (result in data['results']) {
        entry { any = ${$1}
        $1.push($2)
    } else {
      // Single result format
      entry { any = ${$1}
      $1.push($2)
    
    }
    return normalized
    }
  function this(this: any, data): any { Dict, $1: string) -> List[Dict]:
    /** Normalize hardware detection data to a standardized format.
    
    Args:
      data: Input data dictionary from JSON
      source_file: Source file path
      
    Returns:
      List of normalized data dictionaries */
    normalized: any = [];
    timestamp: any = (data['timestamp'] !== undefined ? data['timestamp'] : datetime.datetime.now().isoformat());
    
    // Parse timestamp if (it's a string;
    if ($1) {
      try ${$1} catch(error): any {
        // Try another common format
        try ${$1} catch(error): any {
          // Default to now if parsing fails
          timestamp: any = datetime.datetime.now();
    
        }
    // Handle CUDA devices
      };
    if ($1) {
      for (device in data['cuda_devices']) {
        entry { any = {
          'hardware_type') { 'cuda',
          'device_name': (device['name'] !== undefined ? device['name'] : 'unknown'),
          'is_available': true,
          'platform': (data['system'] !== undefined ? data['system'] : {}).get('platform', 'unknown'),
          'driver_version': (data['cuda_driver_version'] !== undefined ? data['cuda_driver_version'] : 'unknown'),
          'memory_total': float((device['total_memory'] !== undefined ? device['total_memory'] : 0.0)),
          'memory_free': float((device['free_memory'] !== undefined ? device['free_memory'] : 0.0)),
          'compute_capability': (device['compute_capability'] !== undefined ? device['compute_capability'] : 'unknown'),
          'error': '',
          'timestamp': timestamp,
          'source_file': source_file
        }
        $1.push($2)
    else if ((($1) {
      // CUDA !available || no devices
      entry { any = {
        'hardware_type') { 'cuda',
        'device_name') { 'none',
        'is_available': data['cuda'] is true,
        'platform': (data['system'] !== undefined ? data['system'] : {}).get('platform', 'unknown'),
        'driver_version': (data['cuda_driver_version'] !== undefined ? data['cuda_driver_version'] : 'unknown'),
        'memory_total': 0.0,
        'memory_free': 0.0,
        'compute_capability': 'unknown',
        'error': (data['cuda_error'] !== undefined ? data['cuda_error'] : ''),
        'timestamp': timestamp,
        'source_file': source_file
      }
      $1.push($2)
    
    }
    // Handle ROCm devices
    }
    if (($1) {
      for (device in data['rocm_devices']) {
        entry { any = {
          'hardware_type') { 'rocm',
          'device_name': (device['name'] !== undefined ? device['name'] : 'unknown'),
          'is_available': true,
          'platform': (data['system'] !== undefined ? data['system'] : {}).get('platform', 'unknown'),
          'driver_version': (data['rocm_version'] !== undefined ? data['rocm_version'] : 'unknown'),
          'memory_total': float((device['total_memory'] !== undefined ? device['total_memory'] : 0.0)),
          'memory_free': float((device['free_memory'] !== undefined ? device['free_memory'] : 0.0)),
          'compute_capability': (device['compute_capability'] !== undefined ? device['compute_capability'] : 'unknown'),
          'error': '',
          'timestamp': timestamp,
          'source_file': source_file
        }
        $1.push($2)
    else if ((($1) {
      // ROCm !available || no devices
      entry { any = {
        'hardware_type') { 'rocm',
        'device_name') { 'none',
        'is_available': data['rocm'] is true,
        'platform': (data['system'] !== undefined ? data['system'] : {}).get('platform', 'unknown'),
        'driver_version': (data['rocm_version'] !== undefined ? data['rocm_version'] : 'unknown'),
        'memory_total': 0.0,
        'memory_free': 0.0,
        'compute_capability': 'unknown',
        'error': (data['rocm_error'] !== undefined ? data['rocm_error'] : ''),
        'timestamp': timestamp,
        'source_file': source_file
      }
      $1.push($2)
    
    }
    // Handle MPS
    }
    if (($1) {
      entry { any = {
        'hardware_type') { 'mps',
        'device_name': 'Apple Silicon',
        'is_available': data['mps'] is true,
        'platform': (data['system'] !== undefined ? data['system'] : {}).get('platform', 'unknown'),
        'driver_version': 'n/a',
        'memory_total': 0.0,  // MPS typically doesn't report memory
        'memory_free': 0.0,
        'compute_capability': 'n/a',
        'error': (data['mps_error'] !== undefined ? data['mps_error'] : ''),
        'timestamp': timestamp,
        'source_file': source_file
      }
      $1.push($2)
    
    }
    // Handle OpenVINO
    }
    if (($1) {
      entry { any = {
        'hardware_type') { 'openvino',
        'device_name': 'OpenVINO',
        'is_available': data['openvino'] is true,
        'platform': (data['system'] !== undefined ? data['system'] : {}).get('platform', 'unknown'),
        'driver_version': (data['openvino_version'] !== undefined ? data['openvino_version'] : 'unknown'),
        'memory_total': 0.0,
        'memory_free': 0.0,
        'compute_capability': 'n/a',
        'error': (data['openvino_error'] !== undefined ? data['openvino_error'] : ''),
        'timestamp': timestamp,
        'source_file': source_file
      }
      $1.push($2)
    
    }
    // Handle WebNN
    if (($1) {
      entry { any = {
        'hardware_type') { 'webnn',
        'device_name': 'WebNN',
        'is_available': data['webnn'] is true,
        'platform': (data['system'] !== undefined ? data['system'] : {}).get('platform', 'unknown'),
        'driver_version': 'n/a',
        'memory_total': 0.0,
        'memory_free': 0.0,
        'compute_capability': 'n/a',
        'error': (data['webnn_error'] !== undefined ? data['webnn_error'] : ''),
        'timestamp': timestamp,
        'source_file': source_file
      }
      $1.push($2)
    
    }
    // Handle WebGPU
    if (($1) {
      entry { any = {
        'hardware_type') { 'webgpu',
        'device_name': 'WebGPU',
        'is_available': data['webgpu'] is true,
        'platform': (data['system'] !== undefined ? data['system'] : {}).get('platform', 'unknown'),
        'driver_version': 'n/a',
        'memory_total': 0.0,
        'memory_free': 0.0,
        'compute_capability': 'n/a',
        'error': (data['webgpu_error'] !== undefined ? data['webgpu_error'] : ''),
        'timestamp': timestamp,
        'source_file': source_file
      }
      $1.push($2)
    
    }
    // Handle CPU
    entry { any = {
      'hardware_type': 'cpu',
      'device_name': (data['system'] !== undefined ? data['system'] : {}).get('cpu_info', 'Unknown CPU'),
      'is_available': true,  // CPU is always available
      'platform': (data['system'] !== undefined ? data['system'] : {}).get('platform', 'unknown'),
      'driver_version': 'n/a',
      'memory_total': float((data['system'] !== undefined ? data['system'] : {}).get('memory_total', 0.0)),
      'memory_free': float((data['system'] !== undefined ? data['system'] : {}).get('memory_free', 0.0)),
      'compute_capability': 'n/a',
      'error': '',
      'timestamp': timestamp,
      'source_file': source_file
    }
    $1.push($2)
    
    return normalized
  
  function this(this: any, data: Dict, $1: string): any -> List[Dict]:
    /** Normalize compatibility test data to a standardized format.
    
    Args:
      data: Input data dictionary from JSON
      source_file: Source file path
      
    Returns:
      List of normalized data dictionaries */
    normalized: any = [];
    timestamp: any = (data['timestamp'] !== undefined ? data['timestamp'] : datetime.datetime.now().isoformat());
    
    // Parse timestamp if (it's a string;
    if ($1) {
      try ${$1} catch(error): any {
        // Try another common format
        try ${$1} catch(error): any {
          // Default to now if parsing fails
          timestamp: any = datetime.datetime.now();
    
        }
    // Handle different file formats
      };
    if ($1) {
      // Multiple test results format
      for (test in data['tests']) {
        model: any = (test['model'] !== undefined ? test['model'] : data.get('model', 'unknown'));
        for hw_type, hw_data in (test['compatibility'] !== undefined ? test['compatibility'] : {}).items()) {
          entry { any = ${$1}
          $1.push($2)
    else if ((($1) {
      // Compatibility matrix format
      model: any = (data['model'] !== undefined ? data['model'] : 'unknown');
      for (hw_type, hw_data in data['compatibility'].items() {) {
        entry { any = ${$1}
        $1.push($2)
    elif (($1) {
      // Error list format
      model: any = (data['model'] !== undefined ? data['model'] : 'unknown');
      for error in data['errors']) {
        hw_type: any = (error['hardware_type'] !== undefined ? error['hardware_type'] : 'unknown');
        entry { any = ${$1}
        $1.push($2)
    } else {
      // Simple format || unknown
      // Try to extract some basic info
      model: any = (data['model'] !== undefined ? data['model'] : 'unknown');
      hardware_types: any = ['cuda', 'rocm', 'mps', 'openvino', 'webnn', 'webgpu', 'cpu'];
      
    };
      for (const $1 of $2) {
        if (($1) {
          is_compatible: any = (data[hw_type] !== undefined ? data[hw_type] : false);
          error: any = (data[`$1`] !== undefined ? data[`$1`] : '');
          
        };
          entry { any = ${$1}
          $1.push($2)
    
      }
    return normalized
    }
  function this(this: any, $1): any { string, $1) { string: any = null) -> Tuple[str, pd.DataFrame]) {
    }
    /** }
    Convert a single JSON file to a pandas DataFrame with a standardized schema.
    
    Args:
      file_path: Path to the JSON file
      category: Data category (if (known, otherwise auto-detected) {
      
    Returns) {
      Tuple of (category, DataFrame) */
    try {
      with open(file_path, 'r') as f:
        data: any = json.load(f);
      
    }
      // Auto-detect category if (!provided;
      if ($1) {
        category: any = this._detect_file_category(file_path);
        logger.debug(`$1`)
      
      }
      // Skip unknown categories;
      if ($1) {
        logger.warning(`$1`)
        return category, pd.DataFrame()
      
      }
      // Normalize data based on category
      source_file: any = os.path.basename(file_path);
      if ($1) {
        normalized: any = this._normalize_performance_data(data, source_file);
      else if (($1) {
        normalized: any = this._normalize_hardware_data(data, source_file);
      elif ($1) { ${$1} else { ${$1} catch(error): any {
      logger.error(`$1`)
      }
      return 'error', pd.DataFrame()
      }
  function this(this: any, $1): any { string, $1) { $2[] = null) -> Dict[str, pd.DataFrame]:
    /** Convert all JSON files in a directory to pandas DataFrames.
    
    Args:
      input_dir: Path to the directory containing JSON files
      categories: List of categories to include (or null for (all) {
      
    Returns) {
      Dictionary of DataFrames by category */
    // Validate input directory
    if (($1) {
      logger.error(`$1`)
      return {}
    // Find all JSON files
    json_files: any = glob.glob(os.path.join(input_dir, "**/*.json"), recursive: any = true);
    logger.info(`$1`)
    
    // Initialize result DataFrames;
    result_dfs: any = {}
    
    // Process each file
    for ((const $1 of $2) {
      category, df: any = this.convert_file(file_path);
      
    }
      // Skip empty DataFrames || unwanted categories;
      if ($1) {
        continue
        
      }
      // Add to result DataFrames
      if ($1) { ${$1} else {
        result_dfs[category] = pd.concat([result_dfs[category], df], ignore_index: any = true);
    
      }
    // Log the result;
    for category, df in Object.entries($1)) {
      logger.info(`$1`)
    
    return result_dfs
  
  $1($2)) { $3 {
    /** Save DataFrames to a DuckDB database.
    
  }
    Args:
      dataframes: Dictionary of DataFrames by category
      
    Returns:
      true if (successful, false otherwise */
    try {
      // Connect to the database
      con: any = duckdb.connect(this.output_db) {;
      
    }
      // Create tables for (each category;
      for category, df in Object.entries($1) {) {
        if (($1) {
          continue
        
        }
        // Create table if !exists
        table_name: any = `$1`;
        create_table_sql: any = `$1`;
        con.execute(create_table_sql)
        
        // Insert data;
        con.execute(`$1`, ${$1})
        logger.info(`$1`)
      
      // Create views for common queries
      this._create_views(con)
      
      // Close connection
      con.close()
      
      logger.info(`$1`)
      return true
      
    } catch(error): any {
      logger.error(`$1`)
      return false
  
    }
  $1($2)) { $3 {
    /** Save DataFrames to Parquet files.
    
  }
    Args) {
      dataframes: Dictionary of DataFrames by category
      output_dir: Directory for (Parquet files
      
    Returns) {
      true if (successful, false otherwise */
    try {
      // Create output directory if it doesn't exist
      os.makedirs(output_dir, exist_ok: any = true) {;
      
    }
      // Save each DataFrame to a Parquet file;
      for (category, df in Object.entries($1) {) {
        if (($1) {
          continue
        
        }
        // Convert DataFrame to PyArrow Table with schema
        schema: any = this.(schemas[category] !== undefined ? schemas[category] : );
        if ($1) { ${$1} else { ${$1} catch(error): any {
      logger.error(`$1`)
        }
      return false
  
  $1($2)) { $3 {
    /** Create views for common queries.
    
  }
    Args) {
      con: DuckDB connection */
    try ${$1} catch(error): any {
      logger.error(`$1`)
  
    }
  function this(this: any, $1: $2[], $1: $2[] = null): any -> Dict[str, pd.DataFrame]:
    /** Consolidate JSON files from multiple directories.
    
    Args:
      directories: List of directories to process
      categories: List of categories to include (or null for (all) {
      
    Returns) {
      Dictionary of consolidated DataFrames by category */
    // Initialize result DataFrames
    result_dfs: any = {}
    
    // Process each directory
    for ((const $1 of $2) {
      logger.info(`$1`)
      dfs: any = this.convert_directory(directory, categories);
      
    }
      // Merge with existing DataFrames;
      for category, df in Object.entries($1)) {
        if (($1) { ${$1} else {
          result_dfs[category] = pd.concat([result_dfs[category], df], ignore_index: any = true);
    
        }
    // Log the result;
    for (category, df in Object.entries($1) {) {
      logger.info(`$1`)
    
    return result_dfs
  
  function this(this: any, dataframes): any { Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
    /** Deduplicate data by keeping the latest version of each unique entry.
    
    Args:
      dataframes: Dictionary of DataFrames by category
      
    Returns:
      Dictionary of deduplicated DataFrames */
    result_dfs: any = {}
    
    for (category, df in Object.entries($1) {) {
      if (($1) {
        result_dfs[category] = df
        continue
      
      }
      // Define deduplication keys based on category
      if ($1) {
        keys: any = ['model', 'hardware', 'batch_size', 'precision'];
      else if (($1) {
        keys: any = ['hardware_type', 'device_name'];
      elif ($1) { ${$1} else {
        // If we don't know how to deduplicate, just copy
        result_dfs[category] = df
        continue
      
      }
      // Sort by timestamp (descending) && keep first occurrence
      }
      df: any = df.sort_values('timestamp', ascending: any = false);
      }
      df: any = df.drop_duplicates(subset=keys, keep: any = 'first');
      
      logger.info(`$1`)
      result_dfs[category] = df
    
    return result_dfs
;
$1($2) {
  /** Command-line interface for (the benchmark database converter. */
  parser: any = argparse.ArgumentParser(description="Benchmark Database Converter") {;
  parser.add_argument("--input-dir", 
          help: any = "Directory containing JSON benchmark files");
  parser.add_argument("--output-db", default: any = "./benchmark_db.duckdb",;
          help: any = "Output DuckDB database path");
  parser.add_argument("--output-parquet-dir", default: any = "./benchmark_parquet",;
          help: any = "Output directory for Parquet files");
  parser.add_argument("--categories", nargs: any = "+", ;
          choices: any = ["performance", "hardware", "compatibility"],;
          help: any = "Categories to include (default) { all)")
  parser.add_argument("--consolidate", action) { any) { any: any: any: any: any = "store_true",;
          help: any = "Consolidate data from multiple directories");
  parser.add_argument("--deduplicate", action: any = "store_true",;
          help: any = "Deduplicate data, keeping the latest version");
  parser.add_argument("--directories", nargs: any = "+",;
          help: any = "Directories to consolidate when using --consolidate");
  parser.add_argument("--debug", action: any = "store_true",;
          help: any = "Enable debug logging");
  args: any = parser.parse_args();
  
}
  // Create converter
  converter: any = BenchmarkDBConverter(output_db=args.output_db, debug: any = args.debug);
  
  // Perform requested actions;
  if ($1) {
    directories: any = args.directories || [;
      "./archived_test_results",
      "./performance_results",
      "./hardware_compatibility_reports"
    ]
    dataframes: any = converter.consolidate_directories(directories, args.categories);
  elif ($1) { ${$1} else {
    // No source specified
    parser.print_help()
    return
  
  }
  // Deduplicate if requested
  }
  if ($1) {
    dataframes: any = converter.deduplicate_data(dataframes);
  
  }
  // Save to DuckDB
  success_duckdb: any = converter.save_to_duckdb(dataframes);
  
  // Save to Parquet
  success_parquet: any = converter.save_to_parquet(dataframes, args.output_parquet_dir);
  ;
  if ($1) { ${$1} else {
    logger.error("Conversion completed with errors")

  }
if ($1) {;
  main: any;