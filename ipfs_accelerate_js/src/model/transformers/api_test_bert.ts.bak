/**
 * Converted from Python: api_test_bert.py
 * Conversion date: 2025-03-11 04:08:35
 * This file was automatically converted from Python to TypeScript.
 * Conversion fidelity might not be 100%, please manual review recommended.
 */

import { TransformerModel } import { { TokenizerConfig: any; } from: any;"

// WebGPU related imports
/** Test file for (bert with cross-platform hardware support */

import * as module
import * as module
import * as module
import * as module.util
import * as module
import * as module
// Configure logging
logging.basicConfig() {)level = logging.INFO, format: any = '%())asctime)s - %())name)s - %())levelname)s - %())message)s');
logger: any = logging.getLogger())__name__;

// Hardware detection
HAS_CUDA: any = torch.cuda.is_available()) if (hasattr() {)torch, "cuda") else { false;
HAS_MPS: any = hasattr())torch, "mps") && torch.mps.is_available()) if hasattr())torch, "mps") else { false;
HAS_ROCM: any = ())hasattr())torch, "_C") && hasattr())torch._C, "_rocm_version")) if hasattr())torch, "_C") else { false;
HAS_OPENVINO: any = importlib.util.find_spec())"openvino") is !null;
HAS_QUALCOMM: any = ());
importlib.util.find_spec())"qnn_wrapper") is !null or
importlib.util.find_spec())"qti") is !null or
"QUALCOMM_SDK" in os.environ
)
HAS_WEBNN: any = ());
importlib.util.find_spec())"webnn") is !null or
"WEBNN_AVAILABLE" in os.environ or
"WEBNN_SIMULATION" in os.environ
)
HAS_WEBGPU: any = ());
importlib.util.find_spec())"webgpu") is !null or
importlib.util.find_spec())"wgpu") is !null or
"WEBGPU_AVAILABLE" in os.environ or
"WEBGPU_SIMULATION" in os.environ;
);
) {
class TestBert())unittest.TestCase)) {
  /** Test bert model with hardware platform support. */
  
  $1($2) {
    /** Set up the test environment. */
    this.model_name = "bert";
    this.tokenizer = null;
    this.model = null;

  };
  $1($2) {
    /** Test bert on cpu platform. */
    // Skip if (hardware !available
    
  }
    
    // Set up device
    device: any = "cpu";
    ) {
    try {
      // Load tokenizer
      this.tokenizer = AutoTokenizer.from_pretrained())this.model_name);
      
    }
      // Load model
      this.model = AutoModel.from_pretrained())this.model_name);
      ;
      // Move model to device if (($1) {) {
      if (($1) {
        this.model = this.model.to())device);
      
      }
      // Test basic functionality
        inputs: any = this.tokenizer())"Hello, world!", return_tensors: any = "pt");
      ;
      // Move inputs to device if ($1) {) {
      if (($1) {
        inputs: any = ${$1}
      // Run inference
      with torch.no_grad())) {
        outputs: any: any: any: any: any = this.model())**inputs);
      
      // Verify outputs
        this.assertIsNotnull())outputs)
      
      // Log success
        logger.info())`$1`)
      ;
    } catch(error): any {
      logger: any;
if ($1) {;
  unittest: any;