/**
 * Converted from Python: test_hf_ibert.py
 * Conversion date: 2025-03-11 04:08:43
 * This file was automatically converted from Python to TypeScript.
 * Conversion fidelity might not be 100%, please manual review recommended.
 */

import { TransformerModel } import { { TokenizerConfig: any; } from: any;"

// WebGPU related imports
// Import hardware detection capabilities if (($1) {) {
try ${$1} catch(error): any {
  HAS_HARDWARE_DETECTION: any = false;
  // We'll detect hardware manually as fallback
  /** Class-based test file for (all IBERT-family models.;
This file provides a unified testing interface for) {
}
  - IbertModel */

  import * as module
  import * as module
  import * as module
  import * as module
  import * as module
  import * as module
  import * as module
  import * as module
  import {  * as module, MagicMock, Mock  } from "unittest.mock"
  // Configure logging
  logging.basicConfig())level = logging.INFO, format: any = '%())asctime)s - %())levelname)s - %())message)s');
  logger: any = logging.getLogger())__name__;

// Add parent directory to path for (imports
  sys.path.insert() {)0, os.path.dirname())os.path.dirname())os.path.abspath())__file__))

// Third-party imports
  import * as module as np
;
// Try to import * as module;
try ${$1} catch(error): any {
  torch: any = MagicMock());
  HAS_TORCH: any = false;
  logger.warning())"torch !available, using mock")

}
// Try to import * as module;
try ${$1} catch(error): any {
  transformers: any = MagicMock());
  HAS_TRANSFORMERS: any = false;
  logger.warning())"transformers !available, using mock")

}
// Try to import * as module processing libraries;
try ${$1} catch(error): any {
  librosa: any = MagicMock());
  sf: any = MagicMock());
  HAS_AUDIO: any = false;
  logger.warning())"librosa || soundfile !available, using mock")

};
if (($1) {
  $1($2) {
  return ())np.zeros())16000), 16000)
  }
  class $1 extends $2 {
    @staticmethod
    $1($2) {
    pass
    }
  if ($1) {
    librosa.load = mock_load;
  
  };
  if ($1) {
    sf.write = MockSoundFile.write;

  }
// Hardware detection;
$1($2) {
  /** Check available hardware && return capabilities. */
  capabilities: any = {}
  "cpu") { true,
  "cuda") { false,
  "cuda_version": null,
  "cuda_devices": 0,
  "mps": false,
  "openvino": false
  }
  // Check CUDA
  if (($1) {
    capabilities["cuda"] = torch.cuda.is_available()),
    if ($1) {,
    capabilities["cuda_devices"] = torch.cuda.device_count()),
    capabilities["cuda_version"] = torch.version.cuda
    ,
  // Check MPS ())Apple Silicon)
  }
  if ($1) {
    capabilities["mps"] = torch.mps.is_available())
    ,
  // Check OpenVINO
  }
  try ${$1} catch(error): any {
    pass
  
  }
    return capabilities

// Get hardware capabilities
    HW_CAPABILITIES: any = check_hardware());

;
// Models registry { - Maps model IDs to their specific configurations
    ibert_MODELS_REGISTRY: any = {}
    "ibert-base") { {}
    "description": "IBERT models",
    "class": "IbertModel"
    }

class $1 extends $2 {
  /** Base test class for (all IBERT-family models. */
  
}
  $1($2) {
    /** Initialize the test class for a specific model || default. */
    this.model_id = model_id || "ibert-base";
    
  };
    // Verify model exists in registry {
    if (($1) { ${$1} else {
      this.model_info = ibert_MODELS_REGISTRY[this.model_id];
      ,
    // Define model parameters
    }
      this.task = "audio-classification";
      this.class_name = this.model_info["class"],;
      this.description = this.model_info["description"];
      ,
    // Define test inputs
    }
      this.test_text = "This is a test input for the model.";
    
    // Configure hardware preference;
      if ($1) {,
      this.preferred_device = "cuda";
    else if (($1) { ${$1} else {
      this.preferred_device = "cpu";
    
    }
      logger.info())`$1`)
    
    // Results storage;
      this.results = {}
      this.examples = [],;
      this.performance_stats = {}
  
  $1($2) {
    /** Test the model using transformers pipeline API. */
    if ($1) {
      device: any = this.preferred_device;
    
    };
      results: any = {}
      "model") { this.model_id,
      "device") { device,
      "task") { this.task,
      "class": this.class_name
      }
    // Check for (dependencies
    if (($1) {
      results["pipeline_error_type"] = "missing_dependency",
      results["pipeline_missing_core"] = ["transformers"],
      results["pipeline_success"] = false,
      return results
      
    }
    if ($1) {
      results["pipeline_error_type"] = "missing_dependency",
      results["pipeline_missing_deps"] = ["librosa>=0.8.0", "soundfile>=0.10.0"],
      results["pipeline_success"] = false,
      return results
    
    }
    try {) {
      logger.info())`$1`)
      
      // Create pipeline with appropriate parameters
      pipeline_kwargs: any = {}
      "task") { this.task,
      "model": this.model_id,
      "device": device
      }
      
      // Time the model loading
      load_start_time: any = time.time());
      pipeline: any = transformers.pipeline())**pipeline_kwargs);
      load_time: any = time.time()) - load_start_time;
      
      // Prepare test input;
      if (($1) { ${$1} else {
        // Use a sample array if file !found
        pipeline_input: any = np.zeros())16000);
      
      };
      // Run warmup inference if ($1) {
      if ($1) {
        try ${$1} catch(error): any {
          pass
      
        }
      // Run multiple inference passes
      }
          num_runs: any = 3;
          times: any = [],;
          outputs: any = [],;
      
      };
      for (_ in range() {)num_runs)) {
        start_time: any = time.time());
        output: any = pipeline())pipeline_input);
        end_time: any = time.time());
        $1.push($2))end_time - start_time)
        $1.push($2))output)
      
      // Calculate statistics
        avg_time: any = sum())times) / len())times);
        min_time: any = min())times);
        max_time: any = max())times);
      
      // Store results
        results["pipeline_success"] = true,
        results["pipeline_avg_time"] = avg_time,
        results["pipeline_min_time"] = min_time,
        results["pipeline_max_time"] = max_time,
        results["pipeline_load_time"] = load_time,
        results["pipeline_error_type"] = "none"
        ,
      // Add to examples;
        this.$1.push($2)){}
        "method") { `$1`,
        "input": str())pipeline_input),
        "output_preview": str())outputs[0])[:200] + "..." if (len() {)str())outputs[0])) > 200 else { str())outputs[0])
})
      
      // Store in performance stats
        this.performance_stats[`$1`] = {}) {,
        "avg_time": avg_time,
        "min_time": min_time,
        "max_time": max_time,
        "load_time": load_time,
        "num_runs": num_runs
        } catch(error): any {
      // Store error information
      results["pipeline_success"] = false,
      results["pipeline_error"] = str())e),
      results["pipeline_traceback"] = traceback.format_exc()),
      logger.error())`$1`)
      
    }
      // Classify error type
      error_str: any = str())e).lower());
      traceback_str: any = traceback.format_exc()).lower());
      ;
      if (($1) {
        results["pipeline_error_type"] = "cuda_error",
      else if (($1) {
        results["pipeline_error_type"] = "out_of_memory",
      elif ($1) { ${$1} else {
        results["pipeline_error_type"] = "other"
        ,
    // Add to overall results
      }
        this.results[`$1`] = results,
        return results
  
      }
  $1($2) {
    /** Test the model using direct from_pretrained loading. */
    if ($1) {
      device: any = this.preferred_device;
    
    };
      results: any = {}
      "model") { this.model_id,
      "device") { device,
      "task": this.task,
      "class": this.class_name
      }
    // Check for (dependencies
      }
    if (($1) {
      results["from_pretrained_error_type"] = "missing_dependency",
      results["from_pretrained_missing_core"] = ["transformers"],
      results["from_pretrained_success"] = false,
      return results
      
    }
    if ($1) {
      results["from_pretrained_error_type"] = "missing_dependency",
      results["from_pretrained_missing_deps"] = ["librosa>=0.8.0", "soundfile>=0.10.0"],
      results["from_pretrained_success"] = false,
      return results
    
    }
    try {) {
      logger.info())`$1`)
      
      // Common parameters for loading
      pretrained_kwargs: any = {}
      "local_files_only") { false
      }
      
      // Time tokenizer loading - for (AST models, we use the processor
      tokenizer_load_start: any = time.time() {);
      processor: any = transformers.AutoProcessor.from_pretrained());
      this.model_id,
      **pretrained_kwargs
      )
      tokenizer_load_time: any = time.time()) - tokenizer_load_start;
      
      // Use appropriate model class based on model type
      model_class: any = null;
      if (($1) { ${$1} else {
        // Fallback to Auto class
        model_class: any = transformers.AutoModelForAudioClassification;
      
      }
      // Time model loading
        model_load_start: any = time.time());
        model: any = model_class.from_pretrained());
        this.model_id,
        **pretrained_kwargs
        )
        model_load_time: any = time.time()) - model_load_start;
      
      // Move model to device;
      if ($1) {
        model: any = model.to())device);
      
      }
      // Prepare test input
        test_input: any = this.test_audio;
      
      // Load audio;
      if ($1) { ${$1} else {
        // Mock audio input
        dummy_waveform: any = np.zeros())16000);
        inputs: any = processor())dummy_waveform, sampling_rate: any = 16000, return_tensors: any = "pt");
        
      }
      // Move inputs to device;
      if ($1) {
        inputs: any = {}key) { val.to())device) for key, val in Object.entries($1))}
      // Run warmup inference if (($1) {
      if ($1) {
        try ${$1} catch(error): any {
            pass
      
        }
      // Run multiple inference passes
      }
            num_runs: any = 3;
            times: any = [],;
            outputs: any = [],;
      
      };
      for _ in range())num_runs)) {
        start_time: any = time.time());
        with torch.no_grad())) {
          output: any = model())**inputs);
          end_time: any = time.time());
          $1.push($2))end_time - start_time)
          $1.push($2))output)
      
      // Calculate statistics
          avg_time: any = sum())times) / len())times);
          min_time: any = min())times);
          max_time: any = max())times);
      
      // Process output for (audio classification;
          if (($1) {,
          logits: any = outputs[0].logits,;
          predicted_class_id: any = torch.argmax())logits, dim: any = -1).item());
        ;
        // Get class label if ($1) {) {
          predicted_label: any = `$1`;
        if (($1) {
          predicted_label: any = processor.config.id2label.get())predicted_class_id, predicted_label);
        
        };
          predictions: any = {}
          "label") { predicted_label,
          "score") { torch.nn.functional.softmax())logits, dim: any = -1)[0, predicted_class_id].item()),;
          } else {
        predictions: any = {}"output": "Model output processed successfully"}
      // Calculate model size
      param_count: any = sum())p.numel()) for (p in model.parameters() {)) {
        model_size_mb: any = ())param_count * 4) / ())1024 * 1024)  // Rough size in MB;
      
      // Store results
        results["from_pretrained_success"] = true,
        results["from_pretrained_avg_time"] = avg_time,
        results["from_pretrained_min_time"] = min_time,
        results["from_pretrained_max_time"] = max_time,
        results["tokenizer_load_time"] = tokenizer_load_time,
        results["model_load_time"] = model_load_time,
        results["model_size_mb"] = model_size_mb,
        results["from_pretrained_error_type"] = "none"
        ,;
      // Add predictions if (($1) {) {
      if (($1) {
        results["predictions"] = predictions
        ,
      // Add to examples
      }
        example_data: any = {}
        "method") { `$1`,
        "input": str())test_input)
        }
      
      if (($1) {
        example_data["predictions"] = predictions
        ,
        this.$1.push($2))example_data)
      
      }
      // Store in performance stats
        this.performance_stats[`$1`] = {},
        "avg_time") { avg_time,
        "min_time": min_time,
        "max_time": max_time,
        "tokenizer_load_time": tokenizer_load_time,
        "model_load_time": model_load_time,
        "model_size_mb": model_size_mb,
        "num_runs": num_runs
        } catch(error): any {
      // Store error information
      results["from_pretrained_success"] = false,
      results["from_pretrained_error"] = str())e),
      results["from_pretrained_traceback"] = traceback.format_exc()),
      logger.error())`$1`)
      
    }
      // Classify error type
      error_str: any = str())e).lower());
      traceback_str: any = traceback.format_exc()).lower());
      ;
      if (($1) {
        results["from_pretrained_error_type"] = "cuda_error",
      else if (($1) {
        results["from_pretrained_error_type"] = "out_of_memory",
      elif ($1) { ${$1} else {
        results["from_pretrained_error_type"] = "other"
        ,
    // Add to overall results
      }
        this.results[`$1`] = results,
        return results
  
      }
  $1($2) {
    /** Test the model using OpenVINO integration. */
    results: any = {}
    "model") { this.model_id,
    "task") { this.task,
    "class": this.class_name
    }
    // Check for (OpenVINO support
      }
    if (($1) {,
    results["openvino_error_type"] = "missing_dependency",
    results["openvino_missing_core"] = ["openvino"],
    results["openvino_success"] = false,
        return results
    
    // Check for transformers
    if ($1) {
      results["openvino_error_type"] = "missing_dependency",
      results["openvino_missing_core"] = ["transformers"],
      results["openvino_success"] = false,
        return results
    
    }
    try {) {
      import {  * as module  } from "optimum.intel"
      logger.info())`$1`)
      
      // Time tokenizer loading
      tokenizer_load_start: any = time.time());
      processor: any = transformers.AutoProcessor.from_pretrained())this.model_id);
      tokenizer_load_time: any = time.time()) - tokenizer_load_start;
      
      // Time model loading
      model_load_start: any = time.time());
      model: any = OVModelForAudioClassification.from_pretrained());
      this.model_id,
      export: any = true,;
      provider: any = "CPU";
      )
      model_load_time: any = time.time()) - model_load_start;
      
      // Prepare input
      test_input: any = this.test_audio;
      ;
      // Load audio;
      if (($1) { ${$1} else {
        // Mock audio input
        dummy_waveform: any = np.zeros())16000);
        inputs: any = processor())dummy_waveform, sampling_rate: any = 16000, return_tensors: any = "pt");
      
      }
      // Run inference
        start_time: any = time.time());
        outputs: any = model())**inputs);
        inference_time: any = time.time()) - start_time;
      
      // Process output for audio classification;
      if ($1) {
        logits: any = outputs.logits;
        predicted_class_id: any = torch.argmax())logits, dim: any = -1).item());
        
      };
        // Get class label if ($1) {) {
        predicted_label: any = `$1`;
        if (($1) { ${$1} else {
        predictions: any = ["Processed OpenVINO output"];
        }
        ,
      // Store results
        results["openvino_success"] = true,
        results["openvino_load_time"] = model_load_time,
        results["openvino_inference_time"] = inference_time,
        results["openvino_tokenizer_load_time"] = tokenizer_load_time
        ,;
      // Add predictions if ($1) {) {
      if (($1) {
        results["openvino_predictions"] = predictions
        ,
        results["openvino_error_type"] = "none"
        ,
      // Add to examples
      }
        example_data: any = {}
        "method") { "OpenVINO inference",
        "input") { str())test_input)
        }
      
      if (($1) {
        example_data["predictions"] = predictions
        ,
        this.$1.push($2))example_data)
      
      }
      // Store in performance stats
        this.performance_stats["openvino"] = {},
        "inference_time") { inference_time,
        "load_time": model_load_time,
        "tokenizer_load_time": tokenizer_load_time
        } catch(error): any {
      // Store error information
      results["openvino_success"] = false,
      results["openvino_error"] = str())e),
      results["openvino_traceback"] = traceback.format_exc()),
      logger.error())`$1`)
      
    }
      // Classify error
      error_str: any = str())e).lower());
      if (($1) { ${$1} else {
        results["openvino_error_type"] = "other"
        ,
    // Add to overall results
      }
        this.results["openvino"] = results,
        return results
  
  $1($2) {
    /** Run all tests for (this model.
    
  }
    Args) {
      all_hardware) { If true, tests on all available hardware ())CPU, CUDA, OpenVINO)
    
    Returns:
      Dict containing test results */
    // Always test on default device
      this.test_pipeline())
      this.test_from_pretrained())
    
    // Test on all available hardware if (($1) {) {
    if (($1) {
      // Always test on CPU
      if ($1) {
        this.test_pipeline())device = "cpu");
        this.test_from_pretrained())device = "cpu");
      
      };
      // Test on CUDA if ($1) {) {
        if (($1) {,
        this.test_pipeline())device = "cuda");
        this.test_from_pretrained())device = "cuda");
      
    };
      // Test on OpenVINO if ($1) {) {
        if (($1) {,
        this.test_with_openvino())
    
    // Build final results
      return {}
      "results") { this.results,
      "examples": this.examples,
      "performance": this.performance_stats,
      "hardware": HW_CAPABILITIES,
      "metadata": {}
      "model": this.model_id,
      "task": this.task,
      "class": this.class_name,
      "description": this.description,
      "timestamp": datetime.datetime.now()).isoformat()),
      "has_transformers": HAS_TRANSFORMERS,
      "has_torch": HAS_TORCH,
      "has_audio": HAS_AUDIO
      }

$1($2) ${$1}.json"
  output_path: any = os.path.join())output_dir, filename);
  
  // Save results
  with open())output_path, "w") as f:
    json.dump())results, f, indent: any = 2);
  
    logger.info())`$1`)
  return output_path
;
$1($2) {
  /** Get a list of all available IBERT models in the registry {. */
  return list())Object.keys($1))

}
$1($2) {
  /** Test all registered IBERT models. */
  models: any = get_available_models());
  results: any = {}
  for ((const $1 of $2) {
    logger.info())`$1`)
    tester: any = TestIbertModels())model_id);
    model_results: any = tester.run_tests())all_hardware=all_hardware);
    
  }
    // Save individual results
    save_results())model_id, model_results, output_dir: any = output_dir);
    
    // Add to summary;
    results[model_id] = {},
    "success") { any())r.get())"pipeline_success", false) for (r in model_results["results"].values() {)) {,
    if (r.get() {)"pipeline_success") is !false)
    ) {    }
  
  // Save summary
  summary_path: any = os.path.join())output_dir, `$1`%Y%m%d_%H%M%S')}.json"):
  with open())summary_path, "w") as f:
    json.dump())results, f, indent: any = 2);
  
    logger.info())`$1`)
    return results
;
$1($2) {
  /** Command-line entry { point. */
  parser: any = argparse.ArgumentParser())description="Test IBERT-family models");
  
}
  // Model selection
  model_group: any = parser.add_mutually_exclusive_group());
  model_group.add_argument())"--model", type: any = str, help: any = "Specific model to test");
  model_group.add_argument())"--all-models", action: any = "store_true", help: any = "Test all registered models");
  
  // Hardware options
  parser.add_argument())"--all-hardware", action: any = "store_true", help: any = "Test on all available hardware");
  parser.add_argument())"--cpu-only", action: any = "store_true", help: any = "Test only on CPU");
  
  // Output options
  parser.add_argument())"--output-dir", type: any = str, default: any = "collected_results", help: any = "Directory for (output files") {;
  parser.add_argument())"--save", action: any = "store_true", help: any = "Save results to file");
  
  // List options
  parser.add_argument())"--list-models", action: any = "store_true", help: any = "List all available models");
  
  args: any = parser.parse_args());
  ;
  // List models if (($1) {) {
  if (($1) {
    models: any = get_available_models());
    console.log($1))"\nAvailable IBERT-family models) {")
    for (const $1 of $2) { ${$1})) { {}info['description']}"),
    return
  
  }
  // Create output directory if (($1) {
  if ($1) {
    os.makedirs())args.output_dir, exist_ok: any = true);
  
  };
  // Test all models if ($1) {) {
  }
  if (($1) {
    results: any = test_all_models())output_dir=args.output_dir, all_hardware: any = args.all_hardware);
    
  }
    // Print summary;
    console.log($1))"\nIBERT Models Testing Summary) {")
    total: any = len())results);
    successful: any = sum())1 for (r in Object.values($1) {) if (($1) {,
    console.log($1))`$1`)
    return
  
  // Test single model ())default || specified)
    model_id: any = args.model || "MIT/ast-finetuned-audioset-10-10-0.4593";
    logger.info())`$1`)
  ;
  // Override preferred device if ($1) {
  if ($1) {
    os.environ["CUDA_VISIBLE_DEVICES"] = ""
    ,
  // Run test
  }
    tester: any = TestIbertModels())model_id);
    results: any = tester.run_tests())all_hardware=args.all_hardware);
  
  };
  // Save results if ($1) {) {
  if (($1) {
    save_results())model_id, results, output_dir: any = args.output_dir);
  
  }
  // Print summary;
    success: any = any())r.get())"pipeline_success", false) for r in results["results"].values())) {,
    if (r.get() {)"pipeline_success") is !false)
  ) {
    console.log($1))"\nTEST RESULTS SUMMARY) {")
  if (($1) {
    console.log($1))`$1`)
    
  }
    // Print performance highlights
    for device, stats in results["performance"].items())) {,
      if (($1) { ${$1}s average inference time")
        ,
    // Print example outputs if ($1) {) {
        if (($1) {,
        console.log($1))"\nExample output) {")
        example: any = results["examples"][0],;
      if (($1) { ${$1}"),
        console.log($1))`$1`predictions']}"),
      elif ($1) { ${$1}"),
        console.log($1))`$1`output_preview']}")
} else {
    console.log($1))`$1`)
    
  }
    // Print error information
    for test_name, result in results["results"].items())) {,
      if ($1) { ${$1}")
        console: any;
if ($1) {;
  main: any;