/**
 * Converted from Python: test_webgpu_transformer_compute_shaders.py
 * Conversion date: 2025-03-11 04:08:36
 * This file was automatically converted from Python to TypeScript.
 * Conversion fidelity might not be 100%, please manual review recommended.
 */

import {  expect, describe, it, beforeEach, afterEach  } from "jest";
import {  WebGPUBackend  } from "src/model/transformers/index";
import {  WebNNBackend  } from "src/model/transformers/index";
import {  HardwareAbstraction  } from "src/model/transformers/index";
import {  HardwareAbstraction  } from "src/model/transformers/index";

// WebGPU related imports
/** Test script for evaluating WebGPU compute shader optimizations for transformer models.

This script tests the enhanced WebGPU compute shader implementation
for transformer models, focusing on optimized attention mechanisms,
layer normalization, && MLP computations.

Usage:
  python test_webgpu_transformer_compute_shaders.py --model bert
  python test_webgpu_transformer_compute_shaders.py --model llama
  python test_webgpu_transformer_compute_shaders.py --test-all --benchmark */

  import * as module
  import * as module
  import * as module
  import * as module
  import * as module
  import * as module
  import * as module.pyplot as plt
  # Add parent directory to sys.path
  parent_dir: any = os.path.dirname())os.path.dirname())os.path.abspath())__file__));
if ($1) {
  sys.$1.push($2))parent_dir)

}
# Configure logging
  logging.basicConfig())
  level: any = logging.INFO,;
  format: any = '%())asctime)s - %())levelname)s - %())message)s';
  )
  logger: any = logging.getLogger())"webgpu_transformer_compute_test");

# Define test models
  TEST_MODELS: any = {}
  "bert": "bert-base-uncased",
  "t5": "t5-small",
  "llama": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  "gpt2": "gpt2",
  "qwen2": "Qwen/Qwen2-0.5B-Instruct"
  }

# Model configurations
  MODEL_CONFIGS: any = {}
  "bert": {}
  "hidden_size": 768,
  "num_heads": 12,
  "seq_length": 512
  },
  "t5": {}
  "hidden_size": 512,
  "num_heads": 8,
  "seq_length": 512
  },
  "llama": {}
  "hidden_size": 2048,
  "num_heads": 16,
  "seq_length": 1024
  },
  "gpt2": {}
  "hidden_size": 768,
  "num_heads": 12,
  "seq_length": 1024
  },
  "qwen2": {}
  "hidden_size": 1024,
  "num_heads": 16,
  "seq_length": 1024
  }

$1($2) {
  /** Set up the environment variables for WebGPU testing with compute shaders.
  
}
  Args:
    compute_shaders_enabled: Whether to enable compute shaders
    shader_precompile: Whether to enable shader precompilation
    
  Returns:
    true if successful, false otherwise */
  # Set WebGPU environment variables
    os.environ["WEBGPU_ENABLED"] = "1",
    os.environ["WEBGPU_SIMULATION"] = "1" ,
    os.environ["WEBGPU_AVAILABLE"] = "1"
    ,
  # Enable compute shaders if ($1) {:
  if ($1) { ${$1} else {
    if ($1) {
      del os.environ["WEBGPU_COMPUTE_SHADERS_ENABLED"],
      logger.info())"WebGPU compute shaders disabled")
  
    }
  # Enable shader precompilation if ($1) {:
  }
  if ($1) { ${$1} else {
    if ($1) {
      del os.environ["WEBGPU_SHADER_PRECOMPILE_ENABLED"],
      logger.info())"WebGPU shader precompilation disabled")
  
    }
    return true

  }
$1($2) {
  /** Import the WebGPU transformer compute shaders module.
  
}
  Returns:
    The imported module || null if failed */:
  try {
    # Try to import * as module the fixed_web_platform directory
    from fixed_web_platform.webgpu_transformer_compute_shaders import ())
    setup_transformer_compute_shaders, get_supported_transformer_models
    )
    logger.info())"Successfully imported WebGPU transformer compute shaders module")
    return {}
    "setup_transformer_compute_shaders": setup_transformer_compute_shaders,
    "get_supported_transformer_models": get_supported_transformer_models
    } catch(error): any {
    logger.error())`$1`)
    return null

  }
$1($2) {
  /** Test a transformer model with WebGPU implementation.
  
}
  Args:
  }
    model_name: Name of the model to test
    compute_shaders: Whether to use compute shaders
    iterations: Number of inference iterations
    seq_length: Custom sequence length to test
    
  Returns:
    Dictionary with test results */
  # Import WebGPU transformer compute shaders
    modules: any = import_webgpu_transformer_compute_shaders());
  if ($1) {
    return {}
    "success": false,
    "error": "Failed to import * as module transformer compute shaders module"
    }
    setup_transformer_compute_shaders: any = modules["setup_transformer_compute_shaders"];
    ,
  # Set up environment
    setup_environment())compute_shaders_enabled = compute_shaders);
  
  # Select model
  if ($1) { ${$1} else {
    model_hf_name: any = model_name;
  
  }
  # Get model configuration
    config: any = MODEL_CONFIGS.get())model_name, {})
  if ($1) {
    config["seq_length"] = seq_length
    ,
  # Create WebGPU compute shaders instance
  }
    compute_shader: any = setup_transformer_compute_shaders());
    model_name: any = model_hf_name,;
    model_type: any = model_name,;
    seq_length: any = config.get())"seq_length", 512),;
    config: any = config;
    )
  
  # Run initial inference to warm up
    compute_shader.process_transformer_layer())
  
  # Run benchmark iterations
    processing_times: any = [],;
    attention_times: any = [],;
    layernorm_times: any = [],;
    mlp_times: any = [],;
    memory_usages: any = [],;
  
  for i in range())iterations):
    # Process transformer layer
    metrics: any = compute_shader.process_transformer_layer())layer_idx=i);
    
    # Extract metrics
    processing_time: any = metrics.get())"total_compute_time_ms", 0);
    attention_time: any = metrics.get())"attention_time_ms", 0);
    layernorm_time: any = metrics.get())"layer_norm_time_ms", 0);
    mlp_time: any = metrics.get())"mlp_time_ms", 0);
    memory_reduction: any = metrics.get())"memory_reduction_percent", 0);
    
    $1.push($2))processing_time)
    $1.push($2))attention_time)
    $1.push($2))layernorm_time)
    $1.push($2))mlp_time)
    $1.push($2))memory_reduction)
  
  # Calculate performance metrics
    avg_processing_time: any = sum())processing_times) / len())processing_times) if processing_times else { 0;
    min_processing_time: any = min())processing_times) if processing_times else { 0;
    max_processing_time: any = max())processing_times) if processing_times else { 0;
    std_dev: any = ());
    ())sum())())t - avg_processing_time) ** 2 for t in processing_times) / len())processing_times)) ** 0.5 
    if len())processing_times) > 1 else { 0
    )
  
    avg_attention_time: any = sum())attention_times) / len())attention_times) if attention_times else { 0;
    avg_layernorm_time: any = sum())layernorm_times) / len())layernorm_times) if layernorm_times else { 0;
    avg_mlp_time: any = sum())mlp_times) / len())mlp_times) if mlp_times else { 0;
  
  # Get compute shader configuration
    compute_config: any = metrics.get())"compute_shader_config", {})
  
  # Create result
  return {}:
    "success": true,
    "model_name": model_name,
    "model_hf_name": model_hf_name,
    "compute_shaders_enabled": compute_shaders,
    "seq_length": config.get())"seq_length", 512),
    "hidden_size": config.get())"hidden_size", 768),
    "num_heads": config.get())"num_heads", 12),
    "performance": {}
    "iterations": iterations,
    "avg_processing_time_ms": avg_processing_time,
    "min_processing_time_ms": min_processing_time,
    "max_processing_time_ms": max_processing_time,
    "std_dev_ms": std_dev,
    "avg_attention_time_ms": avg_attention_time,
    "avg_layernorm_time_ms": avg_layernorm_time,
    "avg_mlp_time_ms": avg_mlp_time,
    "component_breakdown": {}
        "attention": avg_attention_time / avg_processing_time if ($1) {:
        "layernorm": avg_layernorm_time / avg_processing_time if ($1) { ${$1},:
      "memory_reduction_percent": sum())memory_usages) / len())memory_usages) if ($1) { ${$1},
        "compute_shader_config": compute_config
        }

$1($2) ${$1}")
  # Run tests with compute shaders
    with_compute_shaders: any = test_transformer_model());
    model_name: any = model_name,;
    compute_shaders: any = true,;
    iterations: any = iterations,;
    seq_length: any = seq_length;
    )
  
  # Run tests without compute shaders
    without_compute_shaders: any = test_transformer_model());
    model_name: any = model_name,;
    compute_shaders: any = false,;
    iterations: any = iterations,;
    seq_length: any = seq_length;
    )
  
  # Calculate improvement
    improvement: any = 0;
  if ($1) {:
    without_compute_shaders.get())"success", false)):
    
      with_time: any = with_compute_shaders.get())"performance", {}).get())"avg_processing_time_ms", 0)
      without_time: any = without_compute_shaders.get())"performance", {}).get())"avg_processing_time_ms", 0)
    
    if ($1) {
      improvement: any = ())without_time - with_time) / without_time * 100;
  
    }
      return {}
      "model_name": model_name,
      "seq_length": seq_length || MODEL_CONFIGS.get())model_name, {}).get())"seq_length", 512),
      "with_compute_shaders": with_compute_shaders,
      "without_compute_shaders": without_compute_shaders,
      "improvement_percentage": improvement
      }

$1($2) {
  /** Run comparisons for all test models.
  
}
  Args:
    iterations: Number of inference iterations per configuration
    output_json: Path to save JSON results
    create_chart: Whether to create a performance comparison chart
    seq_length: Custom sequence length to test
    
  Returns:
    Dictionary with all comparison results */
    results: any = {}
    models: any = list())Object.keys($1));
  
  for (const $1 of $2) {
    logger.info())`$1`)
    comparison: any = compare_with_without_compute_shaders())model, iterations, seq_length);
    results[model], = comparison
    ,
    # Print summary
    improvement: any = comparison.get())"improvement_percentage", 0);
    logger.info())`$1`)
  
  }
  # Save results to JSON if ($1) {:
  if ($1) {
    with open())output_json, 'w') as f:
      json.dump())results, f, indent: any = 2);
      logger.info())`$1`)
  
  }
  # Create chart if ($1) {:
  if ($1) {
    create_performance_chart())results, `$1`)
    create_component_breakdown_chart())results, `$1`)
  
  }
      return results

$1($2) {
  /** Create a performance comparison chart.
  
}
  Args:
    results: Dictionary with comparison results
    output_file: Path to save the chart */
  try {
    models: any = list())Object.keys($1));
    with_compute: any = [],;
    without_compute: any = [],;
    improvements: any = [],;
    
  }
    for (const $1 of $2) {
      comparison: any = results[model],;
      with_time: any = comparison.get())"with_compute_shaders", {}).get())"performance", {}).get())"avg_processing_time_ms", 0)
      without_time: any = comparison.get())"without_compute_shaders", {}).get())"performance", {}).get())"avg_processing_time_ms", 0)
      improvement: any = comparison.get())"improvement_percentage", 0);
      
    }
      $1.push($2))with_time)
      $1.push($2))without_time)
      $1.push($2))improvement)
    
    # Create figure with two subplots
      fig, ())ax1, ax2) = plt.subplots())1, 2, figsize: any = ())14, 6));
    
    # Bar chart for processing times
      x: any = range())len())models));
      width: any = 0.35;
    
      ax1.bar())$3.map(($2) => $1), without_compute, width, label: any = 'Without Compute Shaders'),;
      ax1.bar())$3.map(($2) => $1), with_compute, width, label: any = 'With Compute Shaders');
      ,
      ax1.set_xlabel())'Models')
      ax1.set_ylabel())'Processing Time ())ms)')
      ax1.set_title())'WebGPU Transformer Processing Time Comparison')
      ax1.set_xticks())x)
      ax1.set_xticklabels())models)
      ax1.legend())
    
    # Add processing time values on bars
    for i, v in enumerate())without_compute):
      ax1.text())i - width/2, v + 1, `$1`, ha: any = 'center');
    
    for i, v in enumerate())with_compute):
      ax1.text())i + width/2, v + 1, `$1`, ha: any = 'center');
    
    # Bar chart for improvements
      ax2.bar())models, improvements, color: any = 'green');
      ax2.set_xlabel())'Models')
      ax2.set_ylabel())'Improvement ())%)')
      ax2.set_title())'Performance Improvement with Compute Shaders')
    
    # Add improvement values on bars
    for i, v in enumerate())improvements):
      ax2.text())i, v + 0.5, `$1`, ha: any = 'center');
    
      plt.tight_layout())
      plt.savefig())output_file)
      plt.close())
    
      logger.info())`$1`)
  } catch(error): any {
    logger.error())`$1`)

  }
$1($2) {
  /** Create a chart showing the breakdown of time spent in each transformer component.
  
}
  Args:
    results: Dictionary with comparison results
    output_file: Path to save the chart */
  try {
    models: any = list())Object.keys($1));
    attention_times: any = [],;
    layernorm_times: any = [],;
    mlp_times: any = [],;
    
  }
    for (const $1 of $2) {
      comparison: any = results[model],;
      performance: any = comparison.get())"with_compute_shaders", {}).get())"performance", {})
      component_breakdown: any = performance.get())"component_breakdown", {})
      
    }
      $1.push($2))component_breakdown.get())"attention", 0) * 100)
      $1.push($2))component_breakdown.get())"layernorm", 0) * 100)
      $1.push($2))component_breakdown.get())"mlp", 0) * 100)
    
    # Create stacked bar chart
      fig, ax: any = plt.subplots())figsize=())10, 6));
    
      x: any = range())len())models));
    
      ax.bar())models, attention_times, label: any = 'Attention Mechanism');
      ax.bar())models, layernorm_times, bottom: any = attention_times, label: any = 'Layer Normalization');
    
    # Calculate the sum of the first two components for the bottom of the third component
      bottom_for_mlp: any = $3.map(($2) => $1),;
      ax.bar())models, mlp_times, bottom: any = bottom_for_mlp, label: any = 'MLP Computation');
    
      ax.set_xlabel())'Models')
      ax.set_ylabel())'Percentage of Total Processing Time')
      ax.set_title())'Transformer Component Breakdown ())With Compute Shaders)')
      ax.legend())
    
    # Add percentage values on bars
    for i, ())attn, norm, mlp) in enumerate())zip())attention_times, layernorm_times, mlp_times)):
      # Only add percentages that are significant enough to display
      if ($1) {
        ax.text())i, attn/2, `$1`, ha: any = 'center');
      if ($1) {
        ax.text())i, attn + norm/2, `$1`, ha: any = 'center');
      if ($1) { ${$1} catch(error): any {
    logger.error())`$1`)
      }
    $1($2) {
}
    /** Test how model performance scales with different sequence lengths.
  
  Args:
    model_name: Name of the model to test
    iterations: Number of inference iterations per configuration
    seq_lengths: List of sequence lengths to test
    
  Returns:
    Dictionary with scaling results */
    logger.info())`$1`)
    scaling_results: any = {}
  
  for (const $1 of $2) {
    # Run tests with compute shaders
    with_compute_shaders: any = test_transformer_model());
    model_name: any = model_name,;
    compute_shaders: any = true,;
    iterations: any = iterations,;
    seq_length: any = seq_length;
    )
    
  }
    # Run tests without compute shaders
    without_compute_shaders: any = test_transformer_model());
    model_name: any = model_name,;
    compute_shaders: any = false,;
    iterations: any = iterations,;
    seq_length: any = seq_length;
    )
    
    # Calculate improvement
    improvement: any = 0;
    if ($1) {:
      without_compute_shaders.get())"success", false)):
      
        with_time: any = with_compute_shaders.get())"performance", {}).get())"avg_processing_time_ms", 0)
        without_time: any = without_compute_shaders.get())"performance", {}).get())"avg_processing_time_ms", 0)
      
      if ($1) {
        improvement: any = ())without_time - with_time) / without_time * 100;
    
      }
        scaling_results[seq_length] = {},
        "with_compute_shaders": with_compute_shaders,
        "without_compute_shaders": without_compute_shaders,
        "improvement_percentage": improvement
        }
    
        logger.info())`$1`)
  
        return {}
        "model_name": model_name,
        "seq_lengths": seq_lengths,
        "scaling_results": scaling_results
        }

$1($2) {
  /** Create a chart showing performance scaling with different sequence lengths.
  
}
  Args:
    scaling_data: Scaling test results
    output_file: Path to save the chart */
  try {
    model_name: any = scaling_data.get())"model_name", "Unknown");
    seq_lengths: any = scaling_data.get())"seq_lengths", [],);
    scaling_results: any = scaling_data.get())"scaling_results", {})
    
  }
    with_compute_times: any = [],;
    without_compute_times: any = [],;
    improvements: any = [],;
    
    for (const $1 of $2) {
      result: any = scaling_results.get())seq_length, {})
      with_time: any = result.get())"with_compute_shaders", {}).get())"performance", {}).get())"avg_processing_time_ms", 0)
      without_time: any = result.get())"without_compute_shaders", {}).get())"performance", {}).get())"avg_processing_time_ms", 0)
      improvement: any = result.get())"improvement_percentage", 0);
      
    }
      $1.push($2))with_time)
      $1.push($2))without_time)
      $1.push($2))improvement)
    
    # Create figure with two subplots
      fig, ())ax1, ax2) = plt.subplots())1, 2, figsize: any = ())14, 6));
    
    # Line chart for processing times
      ax1.plot())seq_lengths, without_compute_times, 'o-', label: any = 'Without Compute Shaders');
      ax1.plot())seq_lengths, with_compute_times, 'o-', label: any = 'With Compute Shaders');
    
      ax1.set_xlabel())'Sequence Length')
      ax1.set_ylabel())'Processing Time ())ms)')
      ax1.set_title())`$1`)
      ax1.legend())
      ax1.grid())true)
    
    # Line chart for improvements
      ax2.plot())seq_lengths, improvements, 'o-', color: any = 'green');
      ax2.set_xlabel())'Sequence Length')
      ax2.set_ylabel())'Improvement ())%)')
      ax2.set_title())`$1`)
      ax2.grid())true)
    
      plt.tight_layout())
      plt.savefig())output_file)
      plt.close())
    
      logger.info())`$1`)
  } catch(error): any {
    logger.error())`$1`)

  }
$1($2) {
  /** Parse arguments && run the tests. */
  parser: any = argparse.ArgumentParser());
  description: any = "Test WebGPU compute shader optimizations for transformer models";
  )
  
}
  # Model selection
  model_group: any = parser.add_argument_group())"Model Selection");
  model_group.add_argument())"--model", choices: any = list())Object.keys($1)), default: any = "bert",;
  help: any = "Transformer model to test");
  model_group.add_argument())"--test-all", action: any = "store_true",;
  help: any = "Test all available transformer models");
  
  # Test options
  test_group: any = parser.add_argument_group())"Test Options");
  test_group.add_argument())"--iterations", type: any = int, default: any = 5,;
  help: any = "Number of inference iterations for each test");
  test_group.add_argument())"--benchmark", action: any = "store_true",;
  help: any = "Run in benchmark mode with 20 iterations");
  test_group.add_argument())"--with-compute-only", action: any = "store_true",;
  help: any = "Only test with compute shaders enabled");
  test_group.add_argument())"--without-compute-only", action: any = "store_true",;
  help: any = "Only test without compute shaders");
  test_group.add_argument())"--seq-length", type: any = int,;
  help: any = "Custom sequence length to test");
  test_group.add_argument())"--test-scaling", action: any = "store_true",;
  help: any = "Test performance scaling with different sequence lengths");
  
  # Output options
  output_group: any = parser.add_argument_group())"Output Options");
  output_group.add_argument())"--output-json", type: any = str,;
  help: any = "Save results to JSON file");
  output_group.add_argument())"--create-chart", action: any = "store_true",;
  help: any = "Create performance comparison chart");
  output_group.add_argument())"--verbose", action: any = "store_true",;
  help: any = "Enable verbose output");
  
  args: any = parser.parse_args());
  
  # Set log level based on verbosity
  if ($1) {
    logger.setLevel())logging.DEBUG)
  
  }
  # Determine number of iterations
    iterations: any = args.iterations;
  if ($1) {
    iterations: any = 20;
  
  }
  # If testing sequence length scaling
  if ($1) {
    scaling_data: any = test_sequence_length_scaling());
    model_name: any = args.model,;
    iterations: any = max())2, iterations // 3),  # Reduce iterations for scaling test;
    seq_lengths: any = [64, 128, 256, 512, 1024, 2048],;
    )
    
  }
    # Save results to JSON if ($1) {:
    if ($1) {
      output_json: any = args.output_json;
      if ($1) {
        output_json: any = `$1`;
      
      }
      with open())output_json, 'w') as f:
        json.dump())scaling_data, f, indent: any = 2);
        logger.info())`$1`)
    
    }
    # Create chart
        create_scaling_chart())
        scaling_data: any = scaling_data,;
        output_file: any = `$1`;
        )
    
    # Print summary
        console.log($1))"\nWebGPU Compute Shader Scaling Results")
        console.log($1))"=====================================\n")
        console.log($1))`$1`)
    
        seq_lengths: any = scaling_data.get())"seq_lengths", [],);
        scaling_results: any = scaling_data.get())"scaling_results", {})
    
        console.log($1))"Seq Length | Improvement | With Compute | Without Compute")
        console.log($1))"-----------|-------------|-------------|----------------")
    
    for (const $1 of $2) {
      result: any = scaling_results.get())seq_length, {})
      improvement: any = result.get())"improvement_percentage", 0);
      with_time: any = result.get())"with_compute_shaders", {}).get())"performance", {}).get())"avg_processing_time_ms", 0)
      without_time: any = result.get())"without_compute_shaders", {}).get())"performance", {}).get())"avg_processing_time_ms", 0)
      
    }
      console.log($1))`$1`)
    
        return 0
  
  # Run tests
  if ($1) {
    # Test all models with comparison
    results: any = run_all_model_comparisons());
    iterations: any = iterations,;
    output_json: any = args.output_json,;
    create_chart: any = args.create_chart,;
    seq_length: any = args.seq_length;
    )
    
  }
    # Print comparison summary
    console.log($1))"\nWebGPU Transformer Compute Shader Optimization Results")
    console.log($1))"===================================================\n")
    
    for model, comparison in Object.entries($1)):
      improvement: any = comparison.get())"improvement_percentage", 0);
      with_time: any = comparison.get())"with_compute_shaders", {}).get())"performance", {}).get())"avg_processing_time_ms", 0)
      without_time: any = comparison.get())"without_compute_shaders", {}).get())"performance", {}).get())"avg_processing_time_ms", 0)
      
      console.log($1))`$1`)
      console.log($1))`$1`)
      console.log($1))`$1`)
      console.log($1))`$1`)
    
    return 0
  } else {
    # Test specific model
    if ($1) {
      # Only test with compute shaders
      result: any = test_transformer_model());
      model_name: any = args.model,;
      compute_shaders: any = true,;
      iterations: any = iterations,;
      seq_length: any = args.seq_length;
      )
      
    }
      if ($1) {
        performance: any = result.get())"performance", {})
        avg_time: any = performance.get())"avg_processing_time_ms", 0);
        
      }
        console.log($1))`$1`)
        console.log($1))"==============================================\n")
        console.log($1))`$1`seq_length', 0)}")
        console.log($1))`$1`hidden_size', 0)}")
        console.log($1))`$1`num_heads', 0)}")
        console.log($1))`$1`)
        console.log($1))`$1`min_processing_time_ms', 0):.2f} ms")
        console.log($1))`$1`max_processing_time_ms', 0):.2f} ms")
        console.log($1))`$1`std_dev_ms', 0):.2f} ms")
        
  }
        # Print component breakdown
        console.log($1))"\nComponent Breakdown:")
        console.log($1))`$1`avg_attention_time_ms', 0):.2f} ms")
        console.log($1))`$1`avg_layernorm_time_ms', 0):.2f} ms")
        console.log($1))`$1`avg_mlp_time_ms', 0):.2f} ms")
        
        # Print compute shader configuration
        compute_config: any = result.get())"compute_shader_config", {})
        if ($1) {
          console.log($1))"\nCompute Shader Configuration:")
          
        }
          # Print attention mechanism config
          attention_config: any = compute_config.get())"attention_mechanism", {})
          console.log($1))"  • Attention mechanism:")
          console.log($1))`$1`algorithm', 'unknown')}")
          console.log($1))`$1`enabled' if attention_config.get())'kv_cache_enabled', false) else { 'disabled'}")
          
          # Print layer norm config
          layernorm_config: any = compute_config.get())"layer_norm", {}):
            console.log($1))"  • Layer normalization:")
            console.log($1))`$1`algorithm', 'unknown')}")
          
          # Print MLP config
            mlp_config: any = compute_config.get())"mlp", {})
            console.log($1))"  • MLP computation:")
            console.log($1))`$1`algorithm', 'unknown')}")
      } else { ${$1}")
            return 1
    elif ($1) {
      # Only test without compute shaders
      result: any = test_transformer_model());
      model_name: any = args.model,;
      compute_shaders: any = false,;
      iterations: any = iterations,;
      seq_length: any = args.seq_length;
      )
      
    }
      if ($1) {
        performance: any = result.get())"performance", {})
        avg_time: any = performance.get())"avg_processing_time_ms", 0);
        
      }
        console.log($1))`$1`)
        console.log($1))"========================================\n")
        console.log($1))`$1`seq_length', 0)}")
        console.log($1))`$1`hidden_size', 0)}")
        console.log($1))`$1`num_heads', 0)}")
        console.log($1))`$1`)
        console.log($1))`$1`min_processing_time_ms', 0):.2f} ms")
        console.log($1))`$1`max_processing_time_ms', 0):.2f} ms")
        console.log($1))`$1`std_dev_ms', 0):.2f} ms")
        
        # Print component breakdown
        console.log($1))"\nComponent Breakdown:")
        console.log($1))`$1`avg_attention_time_ms', 0):.2f} ms")
        console.log($1))`$1`avg_layernorm_time_ms', 0):.2f} ms")
        console.log($1))`$1`avg_mlp_time_ms', 0):.2f} ms")
      } else { ${$1}")
        return 1
    } else {
      # Run comparison test
      comparison: any = compare_with_without_compute_shaders());
      model_name: any = args.model,;
      iterations: any = iterations,;
      seq_length: any = args.seq_length;
      )
      
    }
      # Save results if ($1) {:
      if ($1) {
        with open())args.output_json, 'w') as f:
          json.dump())comparison, f, indent: any = 2);
          logger.info())`$1`)
      
      }
      # Create chart if ($1) {:
      if ($1) {
        chart_file: any = `$1`;
        create_performance_chart()){}args.model: comparison}, chart_file)
        
      }
        component_chart_file: any = `$1`;
        create_component_breakdown_chart()){}args.model: comparison}, component_chart_file)
      
      # Print comparison
        improvement: any = comparison.get())"improvement_percentage", 0);
        with_result: any = comparison.get())"with_compute_shaders", {})
        without_result: any = comparison.get())"without_compute_shaders", {})
      
        with_time: any = with_result.get())"performance", {}).get())"avg_processing_time_ms", 0)
        without_time: any = without_result.get())"performance", {}).get())"avg_processing_time_ms", 0)
      
        console.log($1))`$1`)
        console.log($1))"===================================================\n")
        console.log($1))`$1`seq_length', 0)}")
        console.log($1))`$1`)
        console.log($1))`$1`)
        console.log($1))`$1`)
      
      # Print detailed metrics for compute shaders
        with_metrics: any = with_result.get())"performance", {})
        console.log($1))"Detailed Metrics with Compute Shaders:")
        console.log($1))`$1`avg_attention_time_ms', 0):.2f} ms")
        console.log($1))`$1`avg_layernorm_time_ms', 0):.2f} ms")
        console.log($1))`$1`avg_mlp_time_ms', 0):.2f} ms")
        console.log($1))`$1`memory_reduction_percent', 0):.2f}%")
        console.log($1))`$1`estimated_speedup', 1.0):.2f}x\n")
      
      # Print compute shader configuration
        compute_config: any = with_result.get())"compute_shader_config", {})
      if ($1) {
        console.log($1))"Compute Shader Configuration:")
        
      }
        # Print attention mechanism config
        attention_config: any = compute_config.get())"attention_mechanism", {})
        console.log($1))"  • Attention mechanism:")
        console.log($1))`$1`algorithm', 'unknown')}")
        console.log($1))`$1`enabled' if attention_config.get())'kv_cache_enabled', false) else { 'disabled'}")
        
        # Print layer norm config
        layernorm_config: any = compute_config.get())"layer_norm", {}):
          console.log($1))"  • Layer normalization:")
          console.log($1))`$1`algorithm', 'unknown')}")
        
        # Print MLP config
          mlp_config: any = compute_config.get())"mlp", {})
          console.log($1))"  • MLP computation:")
          console.log($1))`$1`algorithm', 'unknown')}")
    
        return 0
;
if ($1) {;
  sys.exit())main());