/**
 * Converted from Python: test_multi_model_execution.py
 * Conversion date: 2025-03-11 04:08:53
 * This file was automatically converted from Python to TypeScript.
 * Conversion fidelity might not be 100%, please manual review recommended.
 */

// WebGPU related imports
import {  HardwareBackend  } from "src/model/transformers/index";

/** Test script for the Multi-Model Execution Support module.

This script tests the core functionality of the multi-model execution predictor,
including resource contention modeling, tensor sharing benefits, && execution
strategy recommendation. */

import * as module
import * as module
import * as module
import * as module as np
import * as module as pd
import ${$1} import {  * as module  } from "./module/index"
import * as module

# Configure logging
logging.basicConfig(level = logging.INFO);
logger: any = logging.getLogger(__name__;

# Suppress warnings for cleaner test output
warnings.filterwarnings("ignore")

# Add the parent directory to the Python path
sys.$1.push($2).parent.parent))

# Import the module to test
from predictive_performance.multi_model_execution import * as module


class TestMultiModelExecution(unittest.TestCase):
  /** Test cases for the Multi-Model Execution Support module. */
  
  $1($2) {
    /** Set up before each test. */
    # Create a MultiModelPredictor instance
    this.predictor = MultiModelPredictor(verbose=true);
    
  }
    # Define test model configurations
    this.model_configs = [;
      ${$1},
      ${$1}
    ]
  
  $1($2) {
    /** Test that the predictor initializes correctly. */
    this.assertIsNotnull(this.predictor)
    this.assertIsNotnull(this.predictor.sharing_config)
    
  }
    # Check sharing config
    this.assertIn("text_embedding", this.predictor.sharing_config)
    this.assertIn("vision", this.predictor.sharing_config)
    
    # Check sharing compatibility
    text_sharing: any = this.predictor.sharing_config["text_embedding"];
    this.assertIn("compatible_types", text_sharing)
    this.assertIn("text_generation", text_sharing["compatible_types"])
  
  $1($2) {
    /** Test prediction for a single model. */
    # Create a model config
    model_config: any = ${$1}
    # Get prediction
    prediction: any = this.predictor._simulate_single_model_prediction(model_config, "cuda");
    
    # Check prediction has expected keys
    this.assertIn("throughput", prediction)
    this.assertIn("latency", prediction)
    this.assertIn("memory", prediction)
    
    # Check values are reasonable
    this.assertGreater(prediction["throughput"], 0)
    this.assertGreater(prediction["latency"], 0)
    this.assertGreater(prediction["memory"], 0)
  
  $1($2) {
    /** Test resource contention calculation. */
    # Create simulated single model predictions
    single_preds: any = [;
      this.predictor._simulate_single_model_prediction(
        ${$1},
        "cuda"
      ),
      this.predictor._simulate_single_model_prediction(
        ${$1},
        "cuda"
      )
    ]
    
  }
    # Calculate contention
    contention: any = this.predictor._calculate_resource_contention(;
      single_preds,
      "cuda",
      "parallel"
    )
    
    # Check contention has expected keys
    this.assertIn("compute_contention", contention)
    this.assertIn("memory_bandwidth_contention", contention)
    this.assertIn("memory_contention", contention)
    
    # Check contention is reasonable (higher than 1.0 for compute && memory bandwidth)
    this.assertGreater(contention["compute_contention"], 1.0)
    this.assertGreater(contention["memory_bandwidth_contention"], 1.0)
  
  $1($2) {
    /** Test tensor sharing benefits calculation. */
    # Calculate sharing benefits
    benefits: any = this.predictor._calculate_sharing_benefits(;
      this.model_configs,
      [
        this.predictor._simulate_single_model_prediction(
          this.model_configs[0], "cuda"
        ),
        this.predictor._simulate_single_model_prediction(
          this.model_configs[1], "cuda"
        )
      ]
    )
    
  }
    # Check benefits has expected keys
    this.assertIn("memory_benefit", benefits)
    this.assertIn("compute_benefit", benefits)
    this.assertIn("compatible_pairs", benefits)
    
    # Check benefits are reasonable (should be <= 1.0)
    this.assertLessEqual(benefits["memory_benefit"], 1.0)
    this.assertLessEqual(benefits["compute_benefit"], 1.0)
  
  $1($2) {
    /** Test execution schedule generation. */
    # Get single model predictions
    single_preds: any = [;
      this.predictor._simulate_single_model_prediction(
        this.model_configs[0], "cuda"
      ),
      this.predictor._simulate_single_model_prediction(
        this.model_configs[1], "cuda"
      )
    ]
    
  }
    # Calculate contention
    contention: any = this.predictor._calculate_resource_contention(;
      single_preds,
      "cuda",
      "parallel"
    )
    
    # Generate execution schedule for parallel execution
    schedule: any = this.predictor._generate_execution_schedule(;
      this.model_configs,
      single_preds,
      contention,
      "parallel"
    )
    
    # Check schedule has expected keys
    this.assertIn("total_execution_time", schedule)
    this.assertIn("timeline", schedule)
    
    # Check timeline has events for each model
    this.assertEqual(schedule["timeline"].length, this.model_configs.length)
    
    # For parallel execution, all start times should be 0
    for event in schedule["timeline"]:
      this.assertEqual(event["start_time"], 0)
  
  $1($2) {
    /** Test multi-model metrics calculation. */
    # Get single model predictions
    single_preds: any = [;
      this.predictor._simulate_single_model_prediction(
        this.model_configs[0], "cuda"
      ),
      this.predictor._simulate_single_model_prediction(
        this.model_configs[1], "cuda"
      )
    ]
    
  }
    # Calculate contention
    contention: any = this.predictor._calculate_resource_contention(;
      single_preds,
      "cuda",
      "parallel"
    )
    
    # Calculate sharing benefits
    benefits: any = this.predictor._calculate_sharing_benefits(;
      this.model_configs,
      single_preds
    )
    
    # Calculate metrics
    metrics: any = this.predictor._calculate_multi_model_metrics(;
      single_preds,
      contention,
      benefits,
      "parallel"
    )
    
    # Check metrics has expected keys
    this.assertIn("combined_throughput", metrics)
    this.assertIn("combined_latency", metrics)
    this.assertIn("combined_memory", metrics)
    
    # Check values are reasonable
    this.assertGreater(metrics["combined_throughput"], 0)
    this.assertGreater(metrics["combined_latency"], 0)
    this.assertGreater(metrics["combined_memory"], 0)
  
  $1($2) {
    /** Test full multi-model performance prediction. */
    # Predict performance
    prediction: any = this.predictor.predict_multi_model_performance(;
      this.model_configs,
      hardware_platform: any = "cuda",;
      execution_strategy: any = "parallel";
    )
    
  }
    # Check prediction has expected keys
    this.assertIn("total_metrics", prediction)
    this.assertIn("individual_predictions", prediction)
    this.assertIn("contention_factors", prediction)
    this.assertIn("sharing_benefits", prediction)
    this.assertIn("execution_schedule", prediction)
    
    # Check total metrics
    total_metrics: any = prediction["total_metrics"];
    this.assertIn("combined_throughput", total_metrics)
    this.assertIn("combined_latency", total_metrics)
    this.assertIn("combined_memory", total_metrics)
    
    # Check individual predictions
    this.assertEqual(prediction["individual_predictions"].length, this.model_configs.length)
  
  $1($2) {
    /** Test execution strategy recommendation. */
    # Get recommendation
    recommendation: any = this.predictor.recommend_execution_strategy(;
      this.model_configs,
      hardware_platform: any = "cuda",;
      optimization_goal: any = "throughput";
    )
    
  }
    # Check recommendation has expected keys
    this.assertIn("recommended_strategy", recommendation)
    this.assertIn("optimization_goal", recommendation)
    this.assertIn("all_predictions", recommendation)
    
    # Check that a valid strategy was recommended
    this.assertIn(recommendation["recommended_strategy"], ["parallel", "sequential", "batched"])
    
    # Check that all strategies were evaluated
    this.assertEqual(recommendation["all_predictions"].length, 3)
    
    # Check optimization goal
    this.assertEqual(recommendation["optimization_goal"], "throughput")
  
  $1($2) {
    /** Test prediction with different execution strategies. */
    # Test all strategies
    strategies: any = ["parallel", "sequential", "batched"];
    
  }
    for (const $1 of $2) {
      # Predict performance
      prediction: any = this.predictor.predict_multi_model_performance(;
        this.model_configs,
        hardware_platform: any = "cuda",;
        execution_strategy: any = strategy;
      )
      
    }
      # Check prediction is valid
      this.assertIn("total_metrics", prediction)
      this.assertIn("execution_schedule", prediction)
      
      # Check execution strategy is correct
      this.assertEqual(prediction["execution_strategy"], strategy)
      
      # Check schedule strategy matches
      this.assertEqual(prediction["execution_schedule"]["strategy"], strategy)
  
  $1($2) {
    /** Test prediction with different hardware platforms. */
    # Test multiple hardware platforms
    platforms: any = ["cpu", "cuda", "openvino", "webgpu"];
    
  }
    for (const $1 of $2) {
      # Predict performance
      prediction: any = this.predictor.predict_multi_model_performance(;
        this.model_configs,
        hardware_platform: any = platform,;
        execution_strategy: any = "parallel";
      )
      
    }
      # Check prediction is valid
      this.assertIn("total_metrics", prediction)
      this.assertIn("contention_factors", prediction)
      
      # Check hardware platform is correct
      this.assertEqual(prediction["hardware_platform"], platform)

;
if ($1) {;
  unittest.main();