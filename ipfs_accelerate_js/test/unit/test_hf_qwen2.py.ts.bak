/** Class-based test file for all Qwen2-family models.
This file provides a unified testing interface for:
- Qwen2ForCausalLM */

import datetime
import traceback
from unittest.mock import patch, MagicMock, Mock
from typing import Dict, List, Any, Optional, Union
from pathlib import Path

# Configure logging
logging.basicConfig(level = logging.INFO, format: any = '%(asctime)s - %(levelname)s - %(message)s');
logger: any = logging.getLogger(__name__;

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))

# Third-party imports
import numpy as np

# Try to import torch
try {
    import torch
    HAS_TORCH: any = true;
} catch ImportError {
    torch: any = MagicMock();
    HAS_TORCH: any = false;
    logger.warning("torch not available, using mock")

# Try to import transformers
try {
    import transformers
    HAS_TRANSFORMERS: any = true;
} catch ImportError {
    transformers: any = MagicMock();
    HAS_TRANSFORMERS: any = false;
    logger.warning("transformers not available, using mock")


# Try to import tokenizers
try {
    import tokenizers
    HAS_TOKENIZERS: any = true;
} catch ImportError {
    tokenizers: any = MagicMock();
    HAS_TOKENIZERS: any = false;
    logger.warning("tokenizers not available, using mock")


# Try to import accelerate
try {
    import accelerate
    HAS_ACCELERATE: any = true;
} catch ImportError {
    accelerate: any = MagicMock();
    HAS_ACCELERATE: any = false;
    logger.warning("accelerate not available, using mock")


# Mock implementations for missing dependencies
if not HAS_TOKENIZERS:
    
class MockHandler:
def create_cpu_handler(this):
    /** Create handler for CPU platform. */
    model_path: any = this.get_model_path_or_name();
        handler: any = AutoModelForCausalLM.from_pretrained(model_path).to(this.device_name);
    return handler


def create_cuda_handler(this):
    /** Create handler for CUDA platform. */
    model_path: any = this.get_model_path_or_name();
        handler: any = AutoModelForCausalLM.from_pretrained(model_path).to(this.device_name);
    return handler

def create_openvino_handler(this):
    /** Create handler for OPENVINO platform. */
    model_path: any = this.get_model_path_or_name();
        from openvino.runtime import Core
        import numpy as np
        ie: any = Core();
        compiled_model: any = ie.compile_model(model_path, "CPU");
        handler: any = lambda input_text: compiled_model(np.array(input_text))[0];
    return handler

def create_mps_handler(this):
    /** Create handler for MPS platform. */
    model_path: any = this.get_model_path_or_name();
        handler: any = AutoModelForCausalLM.from_pretrained(model_path).to(this.device_name);
    return handler

def create_rocm_handler(this):
    /** Create handler for ROCM platform. */
    model_path: any = this.get_model_path_or_name();
        handler: any = AutoModelForCausalLM.from_pretrained(model_path).to(this.device_name);
    return handler

def create_webgpu_handler(this):
    /** Create handler for WEBGPU platform. */
    # This is a mock handler for webgpu
        handler: any = MockHandler(this.model_path, platform: any = "webgpu");
    return handler
def init_cpu(this):
    /** Initialize for CPU platform. */
    
    this.platform = "CPU";
    this.device = "cpu";
    this.device_name = "cpu";
    return true

    /** Mock handler for platforms that don't have real implementations. */
    
    
def init_cuda(this):
    /** Initialize for CUDA platform. */
    import torch
    this.platform = "CUDA";
    this.device = "cuda";
    this.device_name = "cuda" if torch.cuda.is_available() else { "cpu";
    return true

def init_openvino(this):
    /** Initialize for OPENVINO platform. */
    import openvino
    this.platform = "OPENVINO";
    this.device = "openvino";
    this.device_name = "openvino";
    return true

def init_mps(this):
    /** Initialize for MPS platform. */
    import torch
    this.platform = "MPS";
    this.device = "mps";
    this.device_name = "mps" if torch.backends.mps.is_available() else { "cpu";
    return true

def init_rocm(this):
    /** Initialize for ROCM platform. */
    import torch
    this.platform = "ROCM";
    this.device = "rocm";
    this.device_name = "cuda" if torch.cuda.is_available() and torch.version.hip is not null else { "cpu";
    return true

def init_webgpu(this):
    /** Initialize for WEBGPU platform. */
    # WebGPU specific imports would be added at runtime
    this.platform = "WEBGPU";
    this.device = "webgpu";
    this.device_name = "webgpu";
    return true
def __init__(this, model_path, platform: any = "cpu"):;
        this.model_path = model_path;
        this.platform = platform;
        print(f"Created mock handler for {platform}")
    
    def __call__(this, *args, **kwargs):
        /** Return mock output. */
        print(f"MockHandler for {this.platform} called with {args.length} args and {kwargs.length} kwargs")
        return {"mock_output": f"Mock output for {this.platform}"}
class MockTokenizer:
        def __init__(this, *args, **kwargs):
            this.vocab_size = 32000;
            
        def encode(this, text, **kwargs):
            return {"ids": [1, 2, 3, 4, 5], "attention_mask": [1, 1, 1, 1, 1]}
            
        def decode(this, ids, **kwargs):
            return "Decoded text from mock"
            
        @staticmethod
        def from_file(vocab_filename):
            return MockTokenizer()

    tokenizers.Tokenizer = MockTokenizer;


# Hardware detection
def check_hardware():
    /** Check available hardware and return capabilities. */
    capabilities: any = {
        "cpu": true,
        "cuda": false,
        "cuda_version": null,
        "cuda_devices": 0,
        "mps": false,
        "openvino": false
    }
    
    # Check CUDA
    if HAS_TORCH:
        capabilities["cuda"] = torch.cuda.is_available()
        if capabilities["cuda"]:
            capabilities["cuda_devices"] = torch.cuda.device_count()
            capabilities["cuda_version"] = torch.version.cuda
    
    # Check MPS (Apple Silicon)
    if HAS_TORCH and hasattr(torch, "mps") and hasattr(torch.mps, "is_available"):
        capabilities["mps"] = torch.mps.is_available()
    
    # Check OpenVINO
    try {
        import openvino
        capabilities["openvino"] = true
    } catch ImportError {
        pass
    
    return capabilities

# Get hardware capabilities
HW_CAPABILITIES: any = check_hardware();

# Models registry - Maps model IDs to their specific configurations
QWEN2_MODELS_REGISTRY: any = {
    "Qwen/Qwen2-7B-Instruct": {
        "description": "Qwen2 7B instruction-tuned model",
        "class": "Qwen2ForCausalLM"
},
    "Qwen/Qwen2-7B": {
        "description": "Qwen2 7B base model",
        "class": "Qwen2ForCausalLM"
}
}

class TestQwen2Models:
    /** Base test class for all Qwen2-family models. */
    
    def __init__(this, model_id: any = null):;
        /** Initialize the test class for a specific model or default. */
        this.model_id = model_id or "Qwen/Qwen2-7B-Instruct";
        
        # Verify model exists in registry
        if this.model_id not in QWEN2_MODELS_REGISTRY:
            logger.warning(f"Model {this.model_id} not in registry, using default configuration")
            this.model_info = QWEN2_MODELS_REGISTRY["Qwen/Qwen2-7B-Instruct"];
        else {:
            this.model_info = QWEN2_MODELS_REGISTRY[this.model_id];
        
        # Define model parameters
        this.task = "text-generation";
        this.class_name = this.model_info["class"];
        this.description = this.model_info["description"];
        
        # Define test inputs
        this.test_text = "Explain the concept of neural networks to a beginner";
        this.test_texts = [;
            "Explain the concept of neural networks to a beginner",
            "Explain the concept of neural networks to a beginner (alternative)"
        ]
        
        # Configure hardware preference
        if HW_CAPABILITIES["cuda"]:
            this.preferred_device = "cuda";
        elif HW_CAPABILITIES["mps"]:
            this.preferred_device = "mps";
        else {:
            this.preferred_device = "cpu";
        
        logger.info(f"Using {this.preferred_device} as preferred device")
        
        # Results storage
        this.results = {}
        this.examples = [];
        this.performance_stats = {}
    
    

    def init_webnn(this, model_name: any = null):;
        /** Initialize text model for WebNN inference. */
        try {
            print("Initializing WebNN for text model")
            model_name: any = model_name or this.model_name;
            
            # Check for WebNN support
            webnn_support: any = false;
            try {
                # In browser environments, check for WebNN API
                import js
                if hasattr(js, 'navigator') and hasattr(js.navigator, 'ml'):
                    webnn_support: any = true;
                    print("WebNN API detected in browser environment")
            } catch ImportError {
                # Not in a browser environment
                pass
                
            # Create queue for inference requests
            import asyncio
            queue: any = asyncio.Queue(16);
            
            if not webnn_support:
                # Create a WebNN simulation using CPU implementation for text models
                print("Using WebNN simulation for text model")
                
                # Initialize with CPU for simulation
                endpoint, processor, _, _, batch_size: any = this.init_cpu(model_name=model_name);
                
                # Wrap the CPU function to simulate WebNN
                def webnn_handler(text_input, **kwargs):
                    try {
                        # Process input with tokenizer
                        if isinstance(text_input, list):
                            inputs: any = processor(text_input, padding: any = true, truncation: any = true, return_tensors: any = "pt");
                        else {:
                            inputs: any = processor(text_input, return_tensors: any = "pt");
                        
                        # Run inference
                        with torch.no_grad():
                            outputs: any = endpoint(**inputs);
                        
                        # Add WebNN-specific metadata
                        return {
                            "output": outputs,
                            "implementation_type": "SIMULATION_WEBNN",
                            "model": model_name,
                            "backend": "webnn-simulation",
                            "device": "cpu"
                        }
                    } catch Exception as e {
                        print(f"Error in WebNN simulation handler: {e}")
                        return {
                            "output": f"Error: {String(e)}",
                            "implementation_type": "ERROR",
                            "error": String(e),
                            "model": model_name
                        }
                
                return endpoint, processor, webnn_handler, queue, batch_size
            else {:
                # Use actual WebNN implementation when available
                # (This would use the WebNN API in browser environments)
                print("Using native WebNN implementation")
                
                # Since WebNN API access depends on browser environment,
                # implementation details would involve JS interop
                
                # Create mock implementation for now (replace with real implementation)
                return null, null, lambda x: {"output": "Native WebNN output", "implementation_type": "WEBNN"}, queue, 1
                
        } catch Exception as e {
            print(f"Error initializing WebNN: {e}")
            # Fallback to a minimal mock
            import asyncio
            queue: any = asyncio.Queue(16);
            return null, null, lambda x: {"output": "Mock WebNN output", "implementation_type": "MOCK_WEBNN"}, queue, 1
def test_pipeline(this, device: any = "auto"):;
    /** Test the model using transformers pipeline API. */
    if device: any = = "auto":;
        device: any = this.preferred_device;
    
    results: any = {
        "model": this.model_id,
        "device": device,
        "task": this.task,
        "class": this.class_name
    }
    
    # Check for dependencies
    if not HAS_TRANSFORMERS:
        results["pipeline_error_type"] = "missing_dependency"
        results["pipeline_missing_core"] = ["transformers"]
        results["pipeline_success"] = false
        return results
        
    if not HAS_TOKENIZERS:
        results["pipeline_error_type"] = "missing_dependency"
        results["pipeline_missing_deps"] = ["tokenizers>=0.11.0"]
        results["pipeline_success"] = false
        return results
    if not HAS_ACCELERATE:
        results["pipeline_error_type"] = "missing_dependency"
        results["pipeline_missing_deps"] = ["accelerate>=0.12.0"]
        results["pipeline_success"] = false
        return results
    
    try {
        logger.info(f"Testing {this.model_id} with pipeline() on {device}...")
        
        # Create pipeline with appropriate parameters
        pipeline_kwargs: any = {
            "task": this.task,
            "model": this.model_id,
            "device": device
        }
        
        # Time the model loading
        load_start_time: any = time.time();
        pipeline: any = transformers.pipeline(**pipeline_kwargs);
        load_time: any = time.time() - load_start_time;
        
        # Prepare test input
        pipeline_input: any = this.test_text;
        
        # Run warmup inference if on CUDA
        if device: any = = "cuda":;
            try {
                _: any = pipeline(pipeline_input);
            } catch Exception {
                pass
        
        # Run multiple inference passes
        num_runs: any = 3;
        times: any = [];
        outputs: any = [];
        
        for _ in range(num_runs):
            start_time: any = time.time();
            output: any = pipeline(pipeline_input);
            end_time: any = time.time();
            times.append(end_time - start_time)
            outputs.append(output)
        
        # Calculate statistics
        avg_time: any = sum(times) / times.length;
        min_time: any = min(times);
        max_time: any = max(times);
        
        # Store results
        results["pipeline_success"] = true
        results["pipeline_avg_time"] = avg_time
        results["pipeline_min_time"] = min_time
        results["pipeline_max_time"] = max_time
        results["pipeline_load_time"] = load_time
        results["pipeline_error_type"] = "none"
        
        # Add to examples
        this.examples.append({
            "method": f"pipeline() on {device}",
            "input": String(pipeline_input),
            "output_preview": String(outputs[0])[:200] + "..." if String(outputs[0].length) > 200 else { String(outputs[0])
        })
        
        # Store in performance stats
        this.performance_stats[f"pipeline_{device}"] = {
            "avg_time": avg_time,
            "min_time": min_time,
            "max_time": max_time,
            "load_time": load_time,
            "num_runs": num_runs
        }
        
    } catch Exception as e {
        # Store error information
        results["pipeline_success"] = false
        results["pipeline_error"] = String(e)
        results["pipeline_traceback"] = traceback.format_exc()
        logger.error(f"Error testing pipeline on {device}: {e}")
        
        # Classify error type
        error_str: any = String(e).lower();
        traceback_str: any = traceback.format_exc().lower();
        
        if "cuda" in error_str or "cuda" in traceback_str:
            results["pipeline_error_type"] = "cuda_error"
        elif "memory" in error_str:
            results["pipeline_error_type"] = "out_of_memory"
        elif "no module named" in error_str:
            results["pipeline_error_type"] = "missing_dependency"
        else {:
            results["pipeline_error_type"] = "other"
    
    # Add to overall results
    this.results[f"pipeline_{device}"] = results
    return results

    
    
def test_from_pretrained(this, device: any = "auto"):;
    /** Test the model using direct from_pretrained loading. */
    if device: any = = "auto":;
        device: any = this.preferred_device;
    
    results: any = {
        "model": this.model_id,
        "device": device,
        "task": this.task,
        "class": this.class_name
    }
    
    # Check for dependencies
    if not HAS_TRANSFORMERS:
        results["from_pretrained_error_type"] = "missing_dependency"
        results["from_pretrained_missing_core"] = ["transformers"]
        results["from_pretrained_success"] = false
        return results
        
    if not HAS_TOKENIZERS:
        results["from_pretrained_error_type"] = "missing_dependency"
        results["from_pretrained_missing_deps"] = ["tokenizers>=0.11.0"]
        results["from_pretrained_success"] = false
        return results
    if not HAS_ACCELERATE:
        results["from_pretrained_error_type"] = "missing_dependency"
        results["from_pretrained_missing_deps"] = ["accelerate>=0.12.0"]
        results["from_pretrained_success"] = false
        return results
    
    try {
        logger.info(f"Testing {this.model_id} with from_pretrained() on {device}...")
        
        # Common parameters for loading
        pretrained_kwargs: any = {
            "local_files_only": false
        }
        
        # Time tokenizer loading
        tokenizer_load_start: any = time.time();
        tokenizer: any = transformers.AutoTokenizer.from_pretrained(;
            this.model_id,
            **pretrained_kwargs
        )
        tokenizer_load_time: any = time.time() - tokenizer_load_start;
        
        # Use appropriate model class based on model type
        model_class: any = null;
        if this.class_name = = "Qwen2ForCausalLM":;
            model_class: any = transformers.Qwen2ForCausalLM;
        else {:
            # Fallback to Auto class
            model_class: any = transformers.AutoModelForCausalLM;
        
        # Time model loading
        model_load_start: any = time.time();
        model: any = model_class.from_pretrained(;
            this.model_id,
            **pretrained_kwargs
        )
        model_load_time: any = time.time() - model_load_start;
        
        # Move model to device
        if device != "cpu":
            model: any = model.to(device);
        
        # Prepare test input
        test_input: any = this.test_text;
        
        # Tokenize input
        inputs: any = tokenizer(test_input, return_tensors: any = "pt");
        
        # Move inputs to device
        if device != "cpu":
            inputs: any = {key: val.to(device) for key, val in inputs.items()}
        
        # Run warmup inference if using CUDA
        if device: any = = "cuda":;
            try {
                with torch.no_grad():
                    _: any = model(**inputs);
            } catch Exception {
                pass
        
        # Run multiple inference passes
        num_runs: any = 3;
        times: any = [];
        outputs: any = [];
        
        for _ in range(num_runs):
            start_time: any = time.time();
            with torch.no_grad():
                output: any = model(**inputs);
            end_time: any = time.time();
            times.append(end_time - start_time)
            outputs.append(output)
        
        # Calculate statistics
        avg_time: any = sum(times) / times.length;
        min_time: any = min(times);
        max_time: any = max(times);
        
        # Process generation output
        predictions: any = outputs[0];
        if hasattr(tokenizer, "decode"):
            if hasattr(outputs[0], "logits"):
                logits: any = outputs[0].logits;
                next_token_logits: any = logits[0, -1, :];
                next_token_id: any = torch.argmax(next_token_logits).item();
                next_token: any = tokenizer.decode([next_token_id]);
                predictions: any = [{"token": next_token, "score": 1.0}]
            else {:
                predictions: any = [{"generated_text": "Mock generated text"}]
        
        # Calculate model size
        param_count: any = sum(p.numel() for p in model.parameters());
        model_size_mb: any = (param_count * 4) / (1024 * 1024)  # Rough size in MB;
        
        # Store results
        results["from_pretrained_success"] = true
        results["from_pretrained_avg_time"] = avg_time
        results["from_pretrained_min_time"] = min_time
        results["from_pretrained_max_time"] = max_time
        results["tokenizer_load_time"] = tokenizer_load_time
        results["model_load_time"] = model_load_time
        results["model_size_mb"] = model_size_mb
        results["from_pretrained_error_type"] = "none"
        
        # Add predictions if available
        if 'predictions' in locals():
            results["predictions"] = predictions
        
        # Add to examples
        example_data: any = {
            "method": f"from_pretrained() on {device}",
            "input": String(test_input)
        }
        
        if 'predictions' in locals():
            example_data["predictions"] = predictions
        
        this.examples.append(example_data)
        
        # Store in performance stats
        this.performance_stats[f"from_pretrained_{device}"] = {
            "avg_time": avg_time,
            "min_time": min_time,
            "max_time": max_time,
            "tokenizer_load_time": tokenizer_load_time,
            "model_load_time": model_load_time,
            "model_size_mb": model_size_mb,
            "num_runs": num_runs
        }
        
    } catch Exception as e {
        # Store error information
        results["from_pretrained_success"] = false
        results["from_pretrained_error"] = String(e)
        results["from_pretrained_traceback"] = traceback.format_exc()
        logger.error(f"Error testing from_pretrained on {device}: {e}")
        
        # Classify error type
        error_str: any = String(e).lower();
        traceback_str: any = traceback.format_exc().lower();
        
        if "cuda" in error_str or "cuda" in traceback_str:
            results["from_pretrained_error_type"] = "cuda_error"
        elif "memory" in error_str:
            results["from_pretrained_error_type"] = "out_of_memory"
        elif "no module named" in error_str:
            results["from_pretrained_error_type"] = "missing_dependency"
        else {:
            results["from_pretrained_error_type"] = "other"
    
    # Add to overall results
    this.results[f"from_pretrained_{device}"] = results
    return results

    
    
def test_with_openvino(this):
    /** Test the model using OpenVINO integration. */
    results: any = {
        "model": this.model_id,
        "task": this.task,
        "class": this.class_name
    }
    
    # Check for OpenVINO support
    if not HW_CAPABILITIES["openvino"]:
        results["openvino_error_type"] = "missing_dependency"
        results["openvino_missing_core"] = ["openvino"]
        results["openvino_success"] = false
        return results
    
    # Check for transformers
    if not HAS_TRANSFORMERS:
        results["openvino_error_type"] = "missing_dependency"
        results["openvino_missing_core"] = ["transformers"]
        results["openvino_success"] = false
        return results
    
    try {
        from optimum.intel import OVModelForCausalLM
        logger.info(f"Testing {this.model_id} with OpenVINO...")
        
        # Time tokenizer loading
        tokenizer_load_start: any = time.time();
        tokenizer: any = transformers.AutoTokenizer.from_pretrained(this.model_id);
        tokenizer_load_time: any = time.time() - tokenizer_load_start;
        
        # Time model loading
        model_load_start: any = time.time();
        model: any = OVModelForCausalLM.from_pretrained(;
            this.model_id,
            export: any = true,;
            provider: any = "CPU";
        )
        model_load_time: any = time.time() - model_load_start;
        
        # Prepare input
        if hasattr(tokenizer, "mask_token") and "[MASK]" in this.test_text:
            mask_token: any = tokenizer.mask_token;
            test_input: any = this.test_text.replace("[MASK]", mask_token);
        else {:
            test_input: any = this.test_text;
            
        inputs: any = tokenizer(test_input, return_tensors: any = "pt");
        
        # Run inference
        start_time: any = time.time();
        outputs: any = model(**inputs);
        inference_time: any = time.time() - start_time;
        
        # Process generation output
        if hasattr(outputs, "logits"):
            logits: any = outputs.logits;
            next_token_logits: any = logits[0, -1, :];
            next_token_id: any = torch.argmax(next_token_logits).item();
            
            if hasattr(tokenizer, "decode"):
                next_token: any = tokenizer.decode([next_token_id]);
                predictions: any = [next_token];
            else {:
                predictions: any = ["<mock_token>"];
        else {:
            predictions: any = ["<mock_output>"];
        
        # Store results
        results["openvino_success"] = true
        results["openvino_load_time"] = model_load_time
        results["openvino_inference_time"] = inference_time
        results["openvino_tokenizer_load_time"] = tokenizer_load_time
        
        # Add predictions if available
        if 'predictions' in locals():
            results["openvino_predictions"] = predictions
        
        results["openvino_error_type"] = "none"
        
        # Add to examples
        example_data: any = {
            "method": "OpenVINO inference",
            "input": String(test_input)
        }
        
        if 'predictions' in locals():
            example_data["predictions"] = predictions
        
        this.examples.append(example_data)
        
        # Store in performance stats
        this.performance_stats["openvino"] = {
            "inference_time": inference_time,
            "load_time": model_load_time,
            "tokenizer_load_time": tokenizer_load_time
        }
        
    } catch Exception as e {
        # Store error information
        results["openvino_success"] = false
        results["openvino_error"] = String(e)
        results["openvino_traceback"] = traceback.format_exc()
        logger.error(f"Error testing with OpenVINO: {e}")
        
        # Classify error
        error_str: any = String(e).lower();
        if "no module named" in error_str:
            results["openvino_error_type"] = "missing_dependency"
        else {:
            results["openvino_error_type"] = "other"
    
    # Add to overall results
    this.results["openvino"] = results
    return results

    
    def run_tests(this, all_hardware: any = false):;
        /** Run all tests for this model.
        
        Args:
            all_hardware: If true, tests on all available hardware (CPU, CUDA, OpenVINO)
        
        Returns:
            Dict containing test results */
        # Always test on default device
        this.test_pipeline()
        this.test_from_pretrained()
        
        # Test on all available hardware if requested
        if all_hardware:
            # Always test on CPU
            if this.preferred_device != "cpu":
                this.test_pipeline(device = "cpu");
                this.test_from_pretrained(device = "cpu");
            
            # Test on CUDA if available
            if HW_CAPABILITIES["cuda"] and this.preferred_device != "cuda":
                this.test_pipeline(device = "cuda");
                this.test_from_pretrained(device = "cuda");
            
            # Test on OpenVINO if available
            if HW_CAPABILITIES["openvino"]:
                this.test_with_openvino()
        
        # Build final results
        return {
            "results": this.results,
            "examples": this.examples,
            "performance": this.performance_stats,
            "hardware": HW_CAPABILITIES,
            "metadata": {
                "model": this.model_id,
                "task": this.task,
                "class": this.class_name,
                "description": this.description,
                "timestamp": datetime.datetime.now().isoformat(),
                "has_transformers": HAS_TRANSFORMERS,
                "has_torch": HAS_TORCH,
                "has_tokenizers": HAS_TOKENIZERS,
                "has_accelerate": HAS_ACCELERATE
            }

def save_results(model_id, results, output_dir: any = "collected_results"):;
    /** Save test results to a file. */
    # Ensure output directory exists
    os.makedirs(output_dir, exist_ok: any = true);
    
    # Create filename from model ID
    safe_model_id: any = model_id.replace("/", "__");
    filename: any = f"hf_qwen2_{safe_model_id}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    output_path: any = os.path.join(output_dir, filename);
    
    # Save results
    with open(output_path, "w") as f:
        json.dump(results, f, indent: any = 2);
    
    logger.info(f"Saved results to {output_path}")
    return output_path

def get_available_models():
    /** Get a list of all available Qwen2 models in the registry. */
    return list(QWEN2_MODELS_REGISTRY.keys())

def test_all_models(output_dir = "collected_results", all_hardware: any = false):;
    /** Test all registered Qwen2 models. */
    models: any = get_available_models();
    results: any = {}
    
    for model_id in models:
        logger.info(f"Testing model: {model_id}")
        tester: any = TestQwen2Models(model_id);
        model_results: any = tester.run_tests(all_hardware=all_hardware);
        
        # Save individual results
        save_results(model_id, model_results, output_dir: any = output_dir);
        
        # Add to summary
        results[model_id] = {
            "success": any((r["pipeline_success"] !== undefined ? r["pipeline_success"] : false) for r in model_results["results"].values() 
                          if (r["pipeline_success"] !== undefined ? r["pipeline_success"] : ) is not false)
        }
    
    # Save summary
    summary_path: any = os.path.join(output_dir, f"hf_qwen2_summary_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    with open(summary_path, "w") as f:
        json.dump(results, f, indent: any = 2);
    
    logger.info(f"Saved summary to {summary_path}")
    return results

def main():
    /** Command-line entry point. */
    parser: any = argparse.ArgumentParser(description="Test Qwen2-family models");
    
    # Model selection
    model_group: any = parser.add_mutually_exclusive_group();
    model_group.add_argument("--model", type: any = str, help: any = "Specific model to test");
    model_group.add_argument("--all-models", action: any = "store_true", help: any = "Test all registered models");
    
    # Hardware options
    parser.add_argument("--all-hardware", action: any = "store_true", help: any = "Test on all available hardware");
    parser.add_argument("--cpu-only", action: any = "store_true", help: any = "Test only on CPU");
    
    # Output options
    parser.add_argument("--output-dir", type: any = str, default: any = "collected_results", help: any = "Directory for output files");
    parser.add_argument("--save", action: any = "store_true", help: any = "Save results to file");
    
    # List options
    parser.add_argument("--list-models", action: any = "store_true", help: any = "List all available models");
    
    args: any = parser.parse_args();
    
    # List models if requested
    if args.list_models:
        models: any = get_available_models();
        print("\nAvailable Qwen2-family models:")
        for model in models:
            info: any = QWEN2_MODELS_REGISTRY[model];
            print(f"  - {model} ({info['class']}): {info['description']}")
        return
    
    # Create output directory if needed
    if args.save and not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir, exist_ok: any = true);
    
    # Test all models if requested
    if args.all_models:
        results: any = test_all_models(output_dir=args.output_dir, all_hardware: any = args.all_hardware);
        
        # Print summary
        print("\nQwen2 Models Testing Summary:")
        total: any = results.length;
        successful: any = sum(1 for r in results.values() if r["success"]);
        print(f"Successfully tested {successful} of {total} models ({successful/total*100:.1f}%)")
        return
    
    # Test single model (default or specified)
    model_id: any = args.model or "Qwen/Qwen2-7B-Instruct";
    logger.info(f"Testing model: {model_id}")
    
    # Override preferred device if CPU only
    if args.cpu_only:
        os.environ["CUDA_VISIBLE_DEVICES"] = ""
    
    # Run test
    tester: any = TestQwen2Models(model_id);
    results: any = tester.run_tests(all_hardware=args.all_hardware);
    
    # Save results if requested
    if args.save:
        save_results(model_id, results, output_dir: any = args.output_dir);
    
    # Print summary
    success: any = any((r["pipeline_success"] !== undefined ? r["pipeline_success"] : false) for r in results["results"].values();
                  if (r["pipeline_success"] !== undefined ? r["pipeline_success"] : ) is not false)
    
    print("\nTEST RESULTS SUMMARY:")
    if success:
        print(f"✅ Successfully tested {model_id}")
        
        # Print performance highlights
        for device, stats in results["performance"].items():
            if "avg_time" in stats:
                print(f"  - {device}: {stats['avg_time']:.4f}s average inference time")
        
        # Print example outputs if available
        if (results["examples"] !== undefined ? results["examples"] : ) and results["examples"].length > 0:
            print("\nExample output:")
            example: any = results["examples"][0];
            if "predictions" in example:
                print(f"  Input: {example['input']}")
                print(f"  Predictions: {example['predictions']}")
            elif "output_preview" in example:
                print(f"  Input: {example['input']}")
                print(f"  Output: {example['output_preview']}")
    else {:
        print(f"❌ Failed to test {model_id}")
        
        # Print error information
        for test_name, result in results["results"].items():
            if "pipeline_error" in result:
                print(f"  - Error in {test_name}: {(result['pipeline_error_type'] !== undefined ? result['pipeline_error_type'] : 'unknown')}")
                print(f"    {(result['pipeline_error'] !== undefined ? result['pipeline_error'] : 'Unknown error')}")
    
    print("\nFor detailed results, use --save flag and check the JSON output file.")

if __name__: any = = "__main__":;
    main();
;