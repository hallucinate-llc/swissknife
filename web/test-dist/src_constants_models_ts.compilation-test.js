"use strict";
/*
 * ATTENTION: The "eval" devtool has been used (maybe by default in mode: "development").
 * This devtool is neither made for production nor for readable output files.
 * It uses "eval()" calls to create a separate source file in the browser devtools.
 * If you are trying to read the output file, select a different devtool (https://webpack.js.org/configuration/devtool/)
 * or disable the default devtool with "devtool: false".
 * If you are looking for production-ready output files, see mode: "production" (https://webpack.js.org/configuration/mode/).
 */
(self["webpackChunkswissknife_web"] = self["webpackChunkswissknife_web"] || []).push([["src_constants_models_ts"],{

/***/ "../src/constants/models.ts":
/*!**********************************!*\
  !*** ../src/constants/models.ts ***!
  \**********************************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   \"default\": () => (__WEBPACK_DEFAULT_EXPORT__),\n/* harmony export */   providers: () => (/* binding */ providers)\n/* harmony export */ });\n/* harmony default export */ const __WEBPACK_DEFAULT_EXPORT__ = ({\n    openai: [\n        {\n            model: 'gpt-4',\n            max_tokens: 4096,\n            max_input_tokens: 8192,\n            max_output_tokens: 4096,\n            input_cost_per_token: 0.00003,\n            output_cost_per_token: 0.00006,\n            provider: 'openai',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_prompt_caching: true,\n            supports_system_messages: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'gpt-4o',\n            max_tokens: 16384,\n            max_input_tokens: 128000,\n            max_output_tokens: 16384,\n            input_cost_per_token: 0.0000025,\n            output_cost_per_token: 0.00001,\n            input_cost_per_token_batches: 0.00000125,\n            output_cost_per_token_batches: 0.000005,\n            cache_read_input_token_cost: 0.00000125,\n            provider: 'openai',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_parallel_function_calling: true,\n            supports_response_schema: true,\n            supports_vision: true,\n            supports_prompt_caching: true,\n            supports_system_messages: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'gpt-4.5-preview',\n            max_tokens: 16384,\n            max_input_tokens: 128000,\n            max_output_tokens: 16384,\n            input_cost_per_token: 0.000075,\n            output_cost_per_token: 0.00015,\n            input_cost_per_token_batches: 0.0000375,\n            output_cost_per_token_batches: 0.000075,\n            cache_read_input_token_cost: 0.0000375,\n            provider: 'openai',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_parallel_function_calling: true,\n            supports_response_schema: true,\n            supports_vision: true,\n            supports_prompt_caching: true,\n            supports_system_messages: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'gpt-4.5-preview-2025-02-27',\n            max_tokens: 16384,\n            max_input_tokens: 128000,\n            max_output_tokens: 16384,\n            input_cost_per_token: 0.000075,\n            output_cost_per_token: 0.00015,\n            input_cost_per_token_batches: 0.0000375,\n            output_cost_per_token_batches: 0.000075,\n            cache_read_input_token_cost: 0.0000375,\n            provider: 'openai',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_parallel_function_calling: true,\n            supports_response_schema: true,\n            supports_vision: true,\n            supports_prompt_caching: true,\n            supports_system_messages: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'gpt-4o-mini',\n            max_tokens: 16384,\n            max_input_tokens: 128000,\n            max_output_tokens: 16384,\n            input_cost_per_token: 1.5e-7,\n            output_cost_per_token: 6e-7,\n            input_cost_per_token_batches: 7.5e-8,\n            output_cost_per_token_batches: 3e-7,\n            cache_read_input_token_cost: 7.5e-8,\n            provider: 'openai',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_parallel_function_calling: true,\n            supports_response_schema: true,\n            supports_vision: true,\n            supports_prompt_caching: true,\n            supports_system_messages: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'gpt-4o-mini-2024-07-18',\n            max_tokens: 16384,\n            max_input_tokens: 128000,\n            max_output_tokens: 16384,\n            input_cost_per_token: 1.5e-7,\n            output_cost_per_token: 6e-7,\n            input_cost_per_token_batches: 7.5e-8,\n            output_cost_per_token_batches: 3e-7,\n            cache_read_input_token_cost: 7.5e-8,\n            provider: 'openai',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_parallel_function_calling: true,\n            supports_response_schema: true,\n            supports_vision: true,\n            supports_prompt_caching: true,\n            supports_system_messages: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'o1',\n            max_tokens: 100000,\n            max_input_tokens: 200000,\n            max_output_tokens: 100000,\n            input_cost_per_token: 0.000015,\n            output_cost_per_token: 0.00006,\n            cache_read_input_token_cost: 0.0000075,\n            provider: 'openai',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_parallel_function_calling: true,\n            supports_vision: true,\n            supports_prompt_caching: true,\n            supports_system_messages: true,\n            supports_response_schema: true,\n            supports_tool_choice: true,\n            supports_reasoning_effort: true,\n        },\n        {\n            model: 'o3-mini',\n            max_tokens: 100000,\n            max_input_tokens: 200000,\n            max_output_tokens: 100000,\n            input_cost_per_token: 0.0000011,\n            output_cost_per_token: 0.0000044,\n            cache_read_input_token_cost: 5.5e-7,\n            provider: 'openai',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_parallel_function_calling: false,\n            supports_vision: false,\n            supports_prompt_caching: true,\n            supports_response_schema: true,\n            supports_tool_choice: true,\n            supports_reasoning_effort: true,\n        },\n        {\n            model: 'o3-mini-2025-01-31',\n            max_tokens: 100000,\n            max_input_tokens: 200000,\n            max_output_tokens: 100000,\n            input_cost_per_token: 0.0000011,\n            output_cost_per_token: 0.0000044,\n            cache_read_input_token_cost: 5.5e-7,\n            provider: 'openai',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_parallel_function_calling: false,\n            supports_vision: false,\n            supports_prompt_caching: true,\n            supports_response_schema: true,\n            supports_tool_choice: true,\n            supports_reasoning_effort: true,\n        },\n        {\n            model: 'o1-2024-12-17',\n            max_tokens: 100000,\n            max_input_tokens: 200000,\n            max_output_tokens: 100000,\n            input_cost_per_token: 0.000015,\n            output_cost_per_token: 0.00006,\n            cache_read_input_token_cost: 0.0000075,\n            provider: 'openai',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_parallel_function_calling: true,\n            supports_vision: true,\n            supports_prompt_caching: true,\n            supports_system_messages: true,\n            supports_response_schema: true,\n            supports_tool_choice: true,\n            supports_reasoning_effort: true,\n        },\n        {\n            model: 'chatgpt-4o-latest',\n            max_tokens: 4096,\n            max_input_tokens: 128000,\n            max_output_tokens: 4096,\n            input_cost_per_token: 0.000005,\n            output_cost_per_token: 0.000015,\n            provider: 'openai',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_parallel_function_calling: true,\n            supports_vision: true,\n            supports_prompt_caching: true,\n            supports_system_messages: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'gpt-4o-2024-05-13',\n            max_tokens: 4096,\n            max_input_tokens: 128000,\n            max_output_tokens: 4096,\n            input_cost_per_token: 0.000005,\n            output_cost_per_token: 0.000015,\n            input_cost_per_token_batches: 0.0000025,\n            output_cost_per_token_batches: 0.0000075,\n            provider: 'openai',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_parallel_function_calling: true,\n            supports_vision: true,\n            supports_prompt_caching: true,\n            supports_system_messages: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'gpt-4o-2024-08-06',\n            max_tokens: 16384,\n            max_input_tokens: 128000,\n            max_output_tokens: 16384,\n            input_cost_per_token: 0.0000025,\n            output_cost_per_token: 0.00001,\n            input_cost_per_token_batches: 0.00000125,\n            output_cost_per_token_batches: 0.000005,\n            cache_read_input_token_cost: 0.00000125,\n            provider: 'openai',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_parallel_function_calling: true,\n            supports_response_schema: true,\n            supports_vision: true,\n            supports_prompt_caching: true,\n            supports_system_messages: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'gpt-4o-2024-11-20',\n            max_tokens: 16384,\n            max_input_tokens: 128000,\n            max_output_tokens: 16384,\n            input_cost_per_token: 0.0000025,\n            output_cost_per_token: 0.00001,\n            input_cost_per_token_batches: 0.00000125,\n            output_cost_per_token_batches: 0.000005,\n            cache_read_input_token_cost: 0.00000125,\n            provider: 'openai',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_parallel_function_calling: true,\n            supports_response_schema: true,\n            supports_vision: true,\n            supports_prompt_caching: true,\n            supports_system_messages: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'gpt-4-turbo',\n            max_tokens: 4096,\n            max_input_tokens: 128000,\n            max_output_tokens: 4096,\n            input_cost_per_token: 0.00001,\n            output_cost_per_token: 0.00003,\n            provider: 'openai',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_parallel_function_calling: true,\n            supports_vision: true,\n            supports_prompt_caching: true,\n            supports_system_messages: true,\n            supports_tool_choice: true,\n        },\n    ],\n    mistral: [\n        {\n            model: 'mistral-small',\n            max_tokens: 8191,\n            max_input_tokens: 32000,\n            max_output_tokens: 8191,\n            input_cost_per_token: 0.000001,\n            output_cost_per_token: 0.000003,\n            provider: 'mistral',\n            supports_function_calling: true,\n            mode: 'chat',\n            supports_assistant_prefill: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'mistral-small-latest',\n            max_tokens: 8191,\n            max_input_tokens: 32000,\n            max_output_tokens: 8191,\n            input_cost_per_token: 0.000001,\n            output_cost_per_token: 0.000003,\n            provider: 'mistral',\n            supports_function_calling: true,\n            mode: 'chat',\n            supports_assistant_prefill: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'mistral-large-latest',\n            max_tokens: 128000,\n            max_input_tokens: 128000,\n            max_output_tokens: 128000,\n            input_cost_per_token: 0.000002,\n            output_cost_per_token: 0.000006,\n            provider: 'mistral',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_assistant_prefill: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'open-mixtral-8x7b',\n            max_tokens: 8191,\n            max_input_tokens: 32000,\n            max_output_tokens: 8191,\n            input_cost_per_token: 7e-7,\n            output_cost_per_token: 7e-7,\n            provider: 'mistral',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_assistant_prefill: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'open-mixtral-8x22b',\n            max_tokens: 8191,\n            max_input_tokens: 65336,\n            max_output_tokens: 8191,\n            input_cost_per_token: 0.000002,\n            output_cost_per_token: 0.000006,\n            provider: 'mistral',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_assistant_prefill: true,\n            supports_tool_choice: true,\n        },\n    ],\n    deepseek: [\n        {\n            model: 'deepseek-reasoner',\n            max_tokens: 8192,\n            max_input_tokens: 65536,\n            max_output_tokens: 8192,\n            input_cost_per_token: 5.5e-7,\n            input_cost_per_token_cache_hit: 1.4e-7,\n            output_cost_per_token: 0.00000219,\n            provider: 'deepseek',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_assistant_prefill: true,\n            supports_tool_choice: true,\n            supports_prompt_caching: true,\n        },\n        {\n            model: 'deepseek-chat',\n            max_tokens: 8192,\n            max_input_tokens: 65536,\n            max_output_tokens: 8192,\n            input_cost_per_token: 2.7e-7,\n            input_cost_per_token_cache_hit: 7e-8,\n            cache_read_input_token_cost: 7e-8,\n            cache_creation_input_token_cost: 0,\n            output_cost_per_token: 0.0000011,\n            provider: 'deepseek',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_assistant_prefill: true,\n            supports_tool_choice: true,\n            supports_prompt_caching: true,\n        },\n        {\n            model: 'deepseek-coder',\n            max_tokens: 4096,\n            max_input_tokens: 128000,\n            max_output_tokens: 4096,\n            input_cost_per_token: 1.4e-7,\n            input_cost_per_token_cache_hit: 1.4e-8,\n            output_cost_per_token: 2.8e-7,\n            provider: 'deepseek',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_assistant_prefill: true,\n            supports_tool_choice: true,\n            supports_prompt_caching: true,\n        },\n    ],\n    xai: [\n        {\n            model: 'grok-beta',\n            max_tokens: 131072,\n            max_input_tokens: 131072,\n            max_output_tokens: 131072,\n            input_cost_per_token: 0.000005,\n            output_cost_per_token: 0.000015,\n            provider: 'xai',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_vision: true,\n            supports_tool_choice: true,\n        },\n    ],\n    groq: [\n        {\n            model: 'llama-3.3-70b-versatile',\n            max_tokens: 8192,\n            max_input_tokens: 128000,\n            max_output_tokens: 8192,\n            input_cost_per_token: 5.9e-7,\n            output_cost_per_token: 7.9e-7,\n            provider: 'groq',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_response_schema: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'llama2-70b-4096',\n            max_tokens: 4096,\n            max_input_tokens: 4096,\n            max_output_tokens: 4096,\n            input_cost_per_token: 7e-7,\n            output_cost_per_token: 8e-7,\n            provider: 'groq',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_response_schema: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'llama3-8b-8192',\n            max_tokens: 8192,\n            max_input_tokens: 8192,\n            max_output_tokens: 8192,\n            input_cost_per_token: 5e-8,\n            output_cost_per_token: 8e-8,\n            provider: 'groq',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_response_schema: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'llama-3.2-1b-preview',\n            max_tokens: 8192,\n            max_input_tokens: 8192,\n            max_output_tokens: 8192,\n            input_cost_per_token: 4e-8,\n            output_cost_per_token: 4e-8,\n            provider: 'groq',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_response_schema: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'llama-3.2-3b-preview',\n            max_tokens: 8192,\n            max_input_tokens: 8192,\n            max_output_tokens: 8192,\n            input_cost_per_token: 6e-8,\n            output_cost_per_token: 6e-8,\n            provider: 'groq',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_response_schema: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'llama-3.2-11b-text-preview',\n            max_tokens: 8192,\n            max_input_tokens: 8192,\n            max_output_tokens: 8192,\n            input_cost_per_token: 1.8e-7,\n            output_cost_per_token: 1.8e-7,\n            provider: 'groq',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_response_schema: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'llama-3.2-90b-text-preview',\n            max_tokens: 8192,\n            max_input_tokens: 8192,\n            max_output_tokens: 8192,\n            input_cost_per_token: 9e-7,\n            output_cost_per_token: 9e-7,\n            provider: 'groq',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_response_schema: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'llama3-70b-8192',\n            max_tokens: 8192,\n            max_input_tokens: 8192,\n            max_output_tokens: 8192,\n            input_cost_per_token: 5.9e-7,\n            output_cost_per_token: 7.9e-7,\n            provider: 'groq',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_response_schema: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'llama-3.1-8b-instant',\n            max_tokens: 8000,\n            max_input_tokens: 8000,\n            max_output_tokens: 8000,\n            input_cost_per_token: 5e-8,\n            output_cost_per_token: 8e-8,\n            provider: 'groq',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_response_schema: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'llama-3.1-70b-versatile',\n            max_tokens: 8000,\n            max_input_tokens: 8000,\n            max_output_tokens: 8000,\n            input_cost_per_token: 5.9e-7,\n            output_cost_per_token: 7.9e-7,\n            provider: 'groq',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_response_schema: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'llama-3.1-405b-reasoning',\n            max_tokens: 8000,\n            max_input_tokens: 8000,\n            max_output_tokens: 8000,\n            input_cost_per_token: 5.9e-7,\n            output_cost_per_token: 7.9e-7,\n            provider: 'groq',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_response_schema: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'mixtral-8x7b-32768',\n            max_tokens: 32768,\n            max_input_tokens: 32768,\n            max_output_tokens: 32768,\n            input_cost_per_token: 2.4e-7,\n            output_cost_per_token: 2.4e-7,\n            provider: 'groq',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_response_schema: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'gemma-7b-it',\n            max_tokens: 8192,\n            max_input_tokens: 8192,\n            max_output_tokens: 8192,\n            input_cost_per_token: 7e-8,\n            output_cost_per_token: 7e-8,\n            provider: 'groq',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_response_schema: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'gemma2-9b-it',\n            max_tokens: 8192,\n            max_input_tokens: 8192,\n            max_output_tokens: 8192,\n            input_cost_per_token: 2e-7,\n            output_cost_per_token: 2e-7,\n            provider: 'groq',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_response_schema: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'llama3-groq-70b-8192-tool-use-preview',\n            max_tokens: 8192,\n            max_input_tokens: 8192,\n            max_output_tokens: 8192,\n            input_cost_per_token: 8.9e-7,\n            output_cost_per_token: 8.9e-7,\n            provider: 'groq',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_response_schema: true,\n            supports_tool_choice: true,\n        },\n        {\n            model: 'llama3-groq-8b-8192-tool-use-preview',\n            max_tokens: 8192,\n            max_input_tokens: 8192,\n            max_output_tokens: 8192,\n            input_cost_per_token: 1.9e-7,\n            output_cost_per_token: 1.9e-7,\n            provider: 'groq',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_response_schema: true,\n            supports_tool_choice: true,\n        },\n    ],\n    anthropic: [\n        {\n            model: 'claude-3-5-haiku-latest',\n            max_tokens: 8192,\n            max_input_tokens: 200000,\n            max_output_tokens: 8192,\n            input_cost_per_token: 0.0000008,\n            output_cost_per_token: 0.000004,\n            cache_creation_input_token_cost: 0.00000125,\n            cache_read_input_token_cost: 1e-7,\n            provider: 'anthropic',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_vision: true,\n            tool_use_system_prompt_tokens: 264,\n            supports_assistant_prefill: true,\n            supports_prompt_caching: true,\n            supports_response_schema: true,\n            deprecation_date: '2025-10-01',\n            supports_tool_choice: true,\n        },\n        {\n            model: 'claude-3-opus-latest',\n            max_tokens: 4096,\n            max_input_tokens: 200000,\n            max_output_tokens: 4096,\n            input_cost_per_token: 0.000015,\n            output_cost_per_token: 0.000075,\n            cache_creation_input_token_cost: 0.00001875,\n            cache_read_input_token_cost: 0.0000015,\n            provider: 'anthropic',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_vision: true,\n            tool_use_system_prompt_tokens: 395,\n            supports_assistant_prefill: true,\n            supports_prompt_caching: true,\n            supports_response_schema: true,\n            deprecation_date: '2025-03-01',\n            supports_tool_choice: true,\n        },\n        {\n            model: 'claude-3-7-sonnet-latest',\n            max_tokens: 8192,\n            max_input_tokens: 200000,\n            max_output_tokens: 8192,\n            input_cost_per_token: 0.000003,\n            output_cost_per_token: 0.000015,\n            cache_creation_input_token_cost: 0.00000375,\n            cache_read_input_token_cost: 3e-7,\n            provider: 'anthropic',\n            mode: 'chat',\n            supports_function_calling: true,\n            supports_vision: true,\n            tool_use_system_prompt_tokens: 159,\n            supports_assistant_prefill: true,\n            supports_prompt_caching: true,\n            supports_response_schema: true,\n            deprecation_date: '2025-06-01',\n            supports_tool_choice: true,\n        },\n    ],\n    gemini: [\n        {\n            model: 'gemini-2.0-flash',\n            max_tokens: 8192,\n            max_input_tokens: 1048576,\n            max_output_tokens: 8192,\n            max_images_per_prompt: 3000,\n            max_videos_per_prompt: 10,\n            max_video_length: 1,\n            max_audio_length_hours: 8.4,\n            max_audio_per_prompt: 1,\n            max_pdf_size_mb: 30,\n            input_cost_per_audio_token: 7e-7,\n            input_cost_per_token: 0.0000001,\n            output_cost_per_token: 0.0000004,\n            provider: 'gemini',\n            mode: 'chat',\n            rpm: 10000,\n            tpm: 10000000,\n            supports_system_messages: true,\n            supports_function_calling: true,\n            supports_vision: true,\n            supports_response_schema: true,\n            supports_audio_output: true,\n            supports_tool_choice: true,\n            source: 'https://ai.google.dev/pricing#2_0flash',\n        },\n        {\n            model: 'gemini-2.0-flash-lite',\n            max_tokens: 8192,\n            max_input_tokens: 1048576,\n            max_output_tokens: 8192,\n            max_images_per_prompt: 3000,\n            max_videos_per_prompt: 10,\n            max_video_length: 1,\n            max_audio_length_hours: 8.4,\n            max_audio_per_prompt: 1,\n            max_pdf_size_mb: 30,\n            input_cost_per_audio_token: 7.5e-8,\n            input_cost_per_token: 0.000000075,\n            output_cost_per_token: 0.0000003,\n            provider: 'gemini',\n            mode: 'chat',\n            rpm: 60000,\n            tpm: 10000000,\n            supports_system_messages: true,\n            supports_function_calling: true,\n            supports_vision: true,\n            supports_response_schema: true,\n            supports_audio_output: false,\n            supports_tool_choice: true,\n            source: 'https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash-lite',\n        },\n        {\n            model: 'gemini-2.0-flash-thinking-exp',\n            max_tokens: 8192,\n            max_input_tokens: 1048576,\n            max_output_tokens: 8192,\n            max_images_per_prompt: 3000,\n            max_videos_per_prompt: 10,\n            max_video_length: 1,\n            max_audio_length_hours: 8.4,\n            max_audio_per_prompt: 1,\n            max_pdf_size_mb: 30,\n            input_cost_per_image: 0,\n            input_cost_per_video_per_second: 0,\n            input_cost_per_audio_per_second: 0,\n            input_cost_per_token: 0,\n            input_cost_per_character: 0,\n            input_cost_per_token_above_128k_tokens: 0,\n            input_cost_per_character_above_128k_tokens: 0,\n            input_cost_per_image_above_128k_tokens: 0,\n            input_cost_per_video_per_second_above_128k_tokens: 0,\n            input_cost_per_audio_per_second_above_128k_tokens: 0,\n            output_cost_per_token: 0,\n            output_cost_per_character: 0,\n            output_cost_per_token_above_128k_tokens: 0,\n            output_cost_per_character_above_128k_tokens: 0,\n            provider: 'gemini',\n            mode: 'chat',\n            supports_system_messages: true,\n            supports_function_calling: true,\n            supports_vision: true,\n            supports_response_schema: true,\n            supports_audio_output: true,\n            tpm: 4000000,\n            rpm: 10,\n            source: 'https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-2.0-flash',\n            supports_tool_choice: true,\n        },\n    ],\n    ollama: [],\n});\nconst providers = {\n    openai: {\n        name: 'OpenAI',\n        baseURL: 'https://api.openai.com/v1',\n    },\n    openrouter: {\n        name: 'OpenRouter',\n        baseURL: 'https://openrouter.ai/api/v1',\n    },\n    //   \"anthropic\": {\n    //     \"name\": \"Anthropic\",\n    //     \"baseURL\": \"https://api.anthropic.com/v1\",\n    //     \"status\": \"wip\"\n    //   },\n    gemini: {\n        name: 'Gemini',\n        baseURL: 'https://generativelanguage.googleapis.com/v1beta/openai',\n    },\n    ollama: {\n        name: 'Ollama',\n        baseURL: 'http://localhost:11434/v1',\n    },\n    mistral: {\n        name: 'Mistral',\n        baseURL: 'https://api.mistral.ai/v1',\n    },\n    deepseek: {\n        name: 'DeepSeek',\n        baseURL: 'https://api.deepseek.com',\n    },\n    xai: {\n        name: 'xAI',\n        baseURL: 'https://api.x.ai/v1',\n    },\n    groq: {\n        name: 'Groq',\n        baseURL: 'https://api.groq.com/openai/v1',\n    },\n    azure: {\n        name: 'Azure OpenAI',\n        baseURL: '', // Will be dynamically constructed based on resource name\n    },\n};\n\n\n//# sourceURL=webpack://swissknife-web/../src/constants/models.ts?");

/***/ })

}]);